<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Apuntes de FSIV]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib/media/favicon.png</url><title>Apuntes de FSIV</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Sun, 17 Nov 2024 20:03:23 GMT</lastBuildDate><atom:link href="lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Sun, 17 Nov 2024 20:03:21 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[5. Modelo Proyectivo]]></title><description><![CDATA[ 
 <br>En está sección se nos plantea el siguiente problema que también vimos en el bloque anterior y nos da los fundamentos para después entender como se reconstruye la escena 3D.<br><br>¿Como recupero un punto 3D de una imagen?<br>Como bien ya sabes al hacer una fotografía la componente de la profundidad se pierde, sin embargo, esta es muy importante.<br><img alt="Pasted image 20241029174230.png" src="añadidos/pasted-image-20241029174230.png"><br>Te recomiendo que para repasar esta parte mires esta explicación en <a data-href="2. La Imagen Digital" href="2.-la-imagen-digital.html" class="internal-link" target="_self" rel="noopener nofollow">2. La Imagen Digital</a>.<br>La novedad es que WOW, podemos representar esto como una matriz.<br><br>El vector resultante estará en coordenadas homogéneas, para obtener el vector no homogéneo se divide por la tercer componente.<br><br>Sin embargo, esto solo funciona en una situación ideal y presenta los siguientes problemas:<br>
<br>
Distorsión de perspectiva: Aunque la proyección en perspectiva es deseable para algunas aplicaciones, no representa la realidad con suficiente precisión cuando se usan lentes reales.

<br>
No considera distorsiones que se producen en cámaras reales, especialmente en cámaras con lentes (como la distorsión radial o tangencial).

<br>
Calibración: El modelo pinhole es demasiado ideal para aplicaciones que requieren geometría precisa, como en visión artificial o reconstrucción 3D.

<br><img alt="Pasted image 20241029191812.png" src="añadidos/pasted-image-20241029191812.png"><br><br>Para mejorar la precisión del modelo pinhole, se introduce el modelo pinhole mejorado, que incluye parámetros adicionales para reflejar mejor las características y distorsiones reales de una cámara. Estos parámetros son esenciales para corregir el modelo básico en aplicaciones que requieren alta precisión, como la visión artificial o la reconstrucción 3D.<br>Mejoras del modelo Pin-Hole:<br>
<br>
Distorsión Radial: Debido a la curvatura de las lentes, los puntos en el borde de la imagen se ven distorsionados, especialmente aquellos alejados del centro óptico. Esta distorsión se modela mediante parámetros , , y en algunos casos , que ajustan el efecto de la distorsión radial en el modelo mejorado.


donde  representa la distancia desde el centro óptico al punto proyectado en la imagen.

<br><img alt="Pasted image 20241029192401.png" src="añadidos/pasted-image-20241029192401.png"><br>
<br>
Distorsión Tangencial: Esta distorsión ocurre debido al desalineamiento de los elementos en la lente. Se modela con los parámetros  y :



<br><img alt="Pasted image 20241029192458.png" src="añadidos/pasted-image-20241029192458.png"><br>
<br>
Parámetros Intrínsecos de la Cámara: Además de las distorsiones, el modelo mejorado incluye parámetros intrínsecos que especifican el centro óptico  y el factor de escala de la imagen en los ejes  y , es decir,  y . Estos parámetros permiten modelar variaciones en la distancia focal en los ejes horizontales y verticales de la cámara.
La matriz de calibración de la cámara es:

donde:

<br> y  representan las distancias focales en los ejes  y  respectivamente,
<br> es el centro de la imagen en el plano de imagen.


<br>Ecuación Final del Modelo Mejorado<br>La proyección de un punto  en el espacio 3D hacia un punto  en la imagen, considerando tanto los efectos de distorsión como los de calibración de cámara, es:<br><br><br>donde se aplican las correcciones de distorsión radial y tangencial para obtener las coordenadas corregidas .<br>Este modelo avanzado compensa los efectos no ideales de las lentes y la cámara, proporcionando una representación de la proyección más precisa y ajustada a la geometría de la cámara real.<br><br>Los parámetros extrínsecos de una cámara definen su posición y orientación en relación al sistema de referencia global. Es decir, describen cómo se encuentra ubicada y orientada la cámara en el espacio 3D en relación a un marco de referencia, permitiéndonos transformar las coordenadas de un punto en el mundo a las coordenadas de la cámara.<br>¿Por Qué Son Importantes los Parámetros Extrínsecos?<br>
Cuando se proyecta un punto desde el espacio 3D hacia una imagen 2D, necesitamos saber desde dónde y en qué dirección está "mirando" la cámara. Los parámetros extrínsecos nos permiten alinear el sistema de coordenadas de la cámara con el sistema de referencia del mundo, esencial en aplicaciones como:<br>
<br>Reconstrucción 3D: para mapear correctamente puntos en el mundo real.
<br>Seguimiento de objetos: para conocer el movimiento de un objeto relativo a la cámara.
<br>Realidad aumentada: para alinear objetos virtuales con el entorno real desde la perspectiva de la cámara.
<br>Desglose de los Parámetros Extrínsecos<br>
<br>
Matriz de Rotación ():

<br>Una matriz de tamaño  que representa la orientación de la cámara en relación al sistema de referencia global.
<br>Aplica una rotación a los puntos del sistema de coordenadas del mundo para alinearlos con la orientación de la cámara.

La matriz de rotación tiene la forma:


<br>
Vector de Traslación ():

<br>Un vector  que indica la posición de la cámara en el espacio 3D en relación al sistema de referencia global.
<br>Traslada el sistema de coordenadas de la cámara de forma que el origen del sistema de referencia global quede en el lugar correcto relativo a la cámara.

El vector de traslación tiene la forma:


<br> Transformación Completa: De Coordenadas del Mundo a Coordenadas de la Cámara<br>
Para convertir un punto en el espacio del mundo  a las coordenadas de la cámara , usamos la siguiente fórmula:<br><br>donde  aplica la rotación y  aplica la traslación. Expandiendo esto:<br><br>Esto transforma un punto  en coordenadas del mundo a coordenadas de la cámara , las cuales se pueden usar luego para la proyección al plano de la imagen.<br>Matriz de Transformación Extrínseca Completa<br>
Combinando la rotación y la traslación en una sola matriz  llamada matriz de transformación extrínseca, podemos escribir:<br><br>donde:<br><br>Esta matriz  permite transformar cualquier punto 3D del espacio del mundo al espacio de la cámara en una única multiplicación de matrices, esencial para realizar transformaciones consistentes y precisas en el análisis de imágenes y aplicaciones en 3D.<br><br>La calibración de la cámara es el proceso de determinar los parámetros intrínsecos y parámetros extrínsecos que relacionan un sistema de coordenadas del mundo con el sistema de la cámara y finalmente con el plano de la imagen. La calibración permite proyectar puntos en el espacio 3D a puntos en la imagen de manera precisa, lo cual es esencial en aplicaciones de visión artificial, reconstrucción 3D y mapeo.<br>El resultado que obtenemos de los parámetros intrísecos y extrínsecos es la matriz proyectiva:<br><br>Lo que hace que encontrar las coordenadas en la imagen se haga de la siguiente manera:<br><br><br>Para el siguiente tema es interesante tener en cuenta esto:<br><br>Donde:<br>
<br> la proyección del punto 3D en la imagen
<br> es la matriz proyectiva
<br> es el vector de coordenadas homogéneas del punto 3D
<br> es la pseudo-inversa de 
<br> es el vector de dirección al punto 3D, (Al ser la pseudo-inversa no te da la magnitud exacta solo la dirección)
<br> es la ecuación de la recta desde el centro óptico de la cámara hasta el punto 3D
<br> es el centro óptico de la cámara
<br> es un parámetro escalar que varía a lo largo de la recta
<br><img alt="Pasted image 20241108180646.png" src="añadidos/pasted-image-20241108180646.png"><br>Se suele utilizar un método que usa "Tableros de Ajedrez", mediante un algoritmo reconoce las esquinas de este tablero y se obtienen todos los parámetros instrínsecos (Matriz de intrinseca , Coeficientes de distorsión ) y extrínsecos (Matriz de Rotación y traslación )<br><img alt="Pasted image 20241108181814.png" src="añadidos/pasted-image-20241108181814.png"><br>Aqui un ejemplo de código de como se hace el algoritmo en python:<br>import cv2
import numpy as np

# Tamaño del tablero (número de esquinas interiores)
pattern_size = (9, 6)  # 9x6 es común, ajústalo según tu tablero

# Tamaño de cada cuadrado en el tablero (por ejemplo, 25 mm)
square_size = 25

# Preparar puntos 3D en el espacio del mundo (0,0,0), (1,0,0), ..., (8,5,0)
objp = np.zeros((pattern_size[0]*pattern_size[1], 3), np.float32)
objp[:,:2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)
objp *= square_size

# Listas para almacenar puntos 3D y 2D
objpoints = []  # Puntos 3D en el espacio del mundo
imgpoints = []  # Puntos 2D en el plano de la imagen

# Cargar imágenes de calibración
images = [f'image{i}.jpg' for i in range(1, 21)]  # Suponiendo 20 imágenes

for fname in images:
    img = cv2.imread(fname)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Encontrar las esquinas del tablero de ajedrez
    ret, corners = cv2.findChessboardCorners(gray, pattern_size, None)

    if ret:
        objpoints.append(objp)
        # Refinar ubicaciones de las esquinas
        corners2 = cv2.cornerSubPix(gray, corners, (11,11), (-1,-1), 
                                    criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001))
        imgpoints.append(corners2)

        # Dibujar y mostrar las esquinas detectadas (opcional)
        img = cv2.drawChessboardCorners(img, pattern_size, corners2, ret)
        cv2.imshow('Corners', img)
        cv2.waitKey(100)

cv2.destroyAllWindows()

# Calibración de la cámara
ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)

# Imprimir los parámetros obtenidos
print("Matriz intrínseca:\n", mtx)
print("Coeficientes de distorsión:\n", dist)
print("Vectores de rotación:\n", rvecs)
print("Vectores de traslación:\n", tvecs)

<br>Rodrigues:<br>Es una fórmula matemática para tranformar la matriz de rotación  en un vector que representa las rotaciones en los 3 ejes, también permite el cambio inverso.<br>import numpy as np
import cv2

# Ejemplo de vector de Rodrigues
r = np.array([np.pi / 4, np.pi / 4, 0])  # Vector de Rodrigues (ángulo en radianes)

# Convertir de vector de Rodrigues a matriz de rotación usando OpenCV
R, _ = cv2.Rodrigues(r)
print("Matriz de rotación obtenida de Rodrigues:")
print(R)

# Convertir de matriz de rotación a vector de Rodrigues usando OpenCV
r_reconstructed, _ = cv2.Rodrigues(R)
print("\nVector de Rodrigues reconstruido de la matriz de rotación:")
print(r_reconstructed)
<br>SolvePnP:<br>SolvePnP (Solve Perspective-n-Point) es un algoritmo utilizado para resolver el problema de estimación de la pose de una cámara con respecto a un conjunto de puntos en el espacio 3D.<br> ¿Qué hace SolvePnP?<br>Dado un conjunto de:<br>
<br>Puntos 3D en el mundo (en un sistema de referencia conocido).
<br>Puntos 2D proyectados en la imagen correspondientes a los puntos 3D.
<br>Matriz intrínseca de la cámara (que incluye parámetros como la distancia focal y el centro óptico).
<br>SolvePnP estima:<br>
<br>La rotación (): Cómo está orientada la cámara respecto al sistema de referencia mundial.
<br>La traslación (): La posición de la cámara en el espacio.
]]></description><link>5.-modelo-proyectivo.html</link><guid isPermaLink="false">5. Modelo Proyectivo.md</guid><pubDate>Sun, 17 Nov 2024 20:02:02 GMT</pubDate><enclosure url="añadidos/pasted-image-20241029174230.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="añadidos/pasted-image-20241029174230.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[1. Módulo de Adquisición]]></title><description><![CDATA[ 
 <br>En esta sección vamos a conocer los distintos elementos hardware que integran el módulo de adquisición SIVA<br><br>La luz es una&nbsp;onda electromagnética capaz de ser percibida por el ojo humano y cuya frecuencia determina su color.<br>Espectro Visible:<br><img alt="Captura desde 2024-10-26 17-50-36.png" src="añadidos/captura-desde-2024-10-26-17-50-36.png"><br><br>Para poder formar una imagen digital capturada del mundo real, vamos a necesitar los siguientes elementos fundamentales:<br>
<br>Una fuente de Luz: 
<br>Un objeto que reciba la luz, la refleje : 
<br>Un sensor o receptor de la luz: 
<br>Siendo la luz total dentro de un periodo de tiempo acotado:<br><br><img alt="Captura desde 2024-10-26 18-00-57.png" src="añadidos/captura-desde-2024-10-26-18-00-57.png"><br><br>El comportamiento de la luz y la captura de imágenes esta directamente relacionada con las propiedades de los objetos en estas.<br>Propiedades:<br>
<br>Absorbentes: Selectividad del espectro, definen el color del objeto.
<br>Reflexivas: Materiales que reflejan parte de la luz que reciben (Especulares o Difusos).
<br>Transmitivas: El paso de la luz a través de ellos (Ópacos, Translucidos y Transparentes).
<br><img alt="Captura desde 2024-10-26 18-06-08.png" src="añadidos/captura-desde-2024-10-26-18-06-08.png"><br><br>Dentro de la captura de imágenes nos pueden interesar distintos tipos de iluminación en función de los parámetros de la escena.<br>Estas pueden ser Natural, Incandescente, Fluorescentes, Estroboscópica, Láser, LED ...<br>Además de las fuentes de luz se pueden aplicar distintas técnicas.<br>
<br>Direccional: Lateral, Coaxial, Campo Oscuro
<br>Difusa
<br>A contraluz
<br>Iluminación Estructurada
<br><br><br>El modelo de lente fina es una simplificación idealizada del comportamiento óptico de una lente. Este modelo asume que la lente tiene un espesor despreciable y que los rayos de luz pasan a través de un solo plano central.<br>Hay que aclarar que se hace uso de una lente convergente.<br>Se rige por esta fórmula:<br><br>Donde:<br>
<br> es la distancia focal
<br> es la distancia real al objeto
<br> es la distancia en la imagen
<br><img alt="Captura desde 2024-10-26 18-31-23.png" src="añadidos/captura-desde-2024-10-26-18-31-23.png"><br>Vamos a denotan  como la proyección del objeto del mundo real al que le estamos echando la foto y  a la proyección de la imagen. <br>(También hay que aclarar que esta definición es para que yo la entienda mejor puedo no estar bien expresado).<br>A partir de aquí vamos a hablar de algunos parámetros de lente:<br>Distancia focal :<br>La distancia focal es la distancia entre el centro de la lente y el punto donde los rayos de luz paralelos al eje óptico se enfocan (el punto focal).<br>Cuanto menor sea la distancia focal mayor dispersión de la imagen habrá, por lo tanto más borrosa.<br><img alt="Captura desde 2024-10-26 18-41-55.png" src="añadidos/captura-desde-2024-10-26-18-41-55.png"><br>Coeficiente de Magnificación :<br>La magnificación o aumento en una lente fina describe cómo cambia el tamaño del objeto al formar su imagen.<br><br>
<br>Si  entonces la imagen ha aumentado en comparación al objeto del mundo real.
<br>Si  entonces la imagen a disminuido en comparación al objeto del mundo real.
<br>Aquí pondré la deducción matemática de la distancia focal a partir del aumento:<br>Tomamos de referencia esta imagen: <br><img alt="Captura desde 2024-10-26 18-31-23.png" src="añadidos/captura-desde-2024-10-26-18-31-23.png"><br>A partir de los cuales se obtienen 2 triángulos semejantes (Ambos angulos son el mismo).<br><img alt="Captura desde 2024-10-26 20-09-08.png" src="añadidos/captura-desde-2024-10-26-20-09-08.png"><br>Aplicando reglas trigonométricas se obtiene que:<br><br><br>Igualamos:<br><br>Despejamos en función de :<br><br><br> se considera despreciable.<br>Diafragma :<br>
<br>
El diafragma es un mecanismo ajustable dentro del lente de una cámara que regula la cantidad de luz que pasa a través de él. Funciona como un orificio circular cuya apertura puede cambiarse para controlar la intensidad de luz que alcanza el sensor o la película. 

<br>
La apertura se mide en términos del número .

<br>
Control de Luz: Al abrir más el diafragma, entra más luz, y al cerrarlo, entra menos luz.

<br>
Profundidad de Campo: Una apertura mayor (número  menor) genera una profundidad de campo más reducida, enfocando solo una parte de la escena, mientras que una apertura pequeña (número  alto) aumenta la profundidad de campo, manteniendo más elementos de la imagen en foco.

<br>Número  :<br>El número , también llamado relación focal o apertura relativa, es una medida de la apertura del diafragma de la lente, representada con la letra f seguida de un número (como f/2.8, f/5.6, etc.). Este número indica la relación entre la distancia focal del lente y el diámetro efectivo de la apertura:<br>Donde:<br>
<br> es la distancia focal.
<br> es el diámetro de la apertura efectiva del diafragma.
<br>Hay que detallar que  sigue una escala geométrica: <br>Aquí voy a poner algunas aplicaciones de distintos usos de números :<br>
<br>
Retratos: Un número  bajo (f/1.4 o f/2.8) es ideal para retratos, ya que produce un desenfoque de fondo que hace que el sujeto destaque.

<br>
Paisajes: Un número  alto (f/8 o superior) es útil en fotografía de paisajes, donde se necesita una profundidad de campo extensa para mantener todos los elementos en foco.

<br>
Fotografía Nocturna: Usar un número  bajo permite captar más luz, ideal en condiciones de baja iluminación.

<br><img alt="Pasted image 20241026191840.png" src="añadidos/pasted-image-20241026191840.png"><br><br>Dentro de los sensores de cámara podemos distinguir 2 tecnologías muy importantes:<br>
<br>
CCD

<br>
CMOS
CCD (Charge-Coupled Device)

<br>
Funcionamiento: Los sensores CCD convierten la luz en carga eléctrica, que se transfiere a través de la matriz de píxeles a un convertidor analógico-digital (ADC) para formar una imagen.

<br>
Calidad de Imagen: Generalmente, los sensores CCD ofrecen una mejor calidad de imagen en términos de ruido, sensibilidad a la luz y rango dinámico. Esto se debe a su diseño que minimiza el ruido electrónico.

<br>
Sensibilidad a la Luz: Tienen una alta sensibilidad, lo que los hace ideales para fotografía en condiciones de poca luz.

<br>
Consumo de Energía: Consume más energía que los sensores CMOS, lo que puede ser un factor limitante en aplicaciones portátiles.

<br><img alt="Captura desde 2024-10-27 09-07-28.png" src="añadidos/captura-desde-2024-10-27-09-07-28.png"><br> Ventajas<br>
<br>Menos Ruido: Producen imágenes con menos ruido, especialmente en situaciones de baja iluminación.
<br>Alta Calidad de Imagen: Proporcionan una mejor uniformidad en la respuesta del sensor, lo que se traduce en una mayor calidad de imagen.
<br>Desventajas<br>
<br>Costo: Generalmente más caros de producir que los sensores CMOS.
<br>Velocidad: Tienen velocidades de lectura más lentas, lo que puede ser una desventaja en aplicaciones de video de alta velocidad o en captura de imágenes en ráfaga.
<br>Aplicaciones<br>
<br>Usados en cámaras profesionales, telescopios, y aplicaciones científicas donde se requiere alta calidad de imagen.
<br>CMOS (Complementary Metal-Oxide-Semiconductor)<br>Características<br>
<br>
Funcionamiento: Los sensores CMOS utilizan transistores individuales para cada píxel, lo que permite que cada píxel convierta la luz en carga eléctrica y la procese directamente.

<br>
Calidad de Imagen: Aunque han mejorado significativamente en calidad de imagen, los sensores CMOS pueden tener más ruido que los CCD en condiciones de poca luz, aunque esto ha mejorado en modelos recientes.

<br>
Consumo de Energía: Más eficientes en términos de consumo de energía, lo que los hace adecuados para dispositivos portátiles.

<br>
Velocidad: Ofrecen velocidades de lectura más rápidas, lo que es beneficioso para video de alta velocidad y captura de imágenes en ráfaga.

<br><img alt="Captura desde 2024-10-27 09-12-27.png" src="añadidos/captura-desde-2024-10-27-09-12-27.png"><br>Ventajas<br>
<br>
Bajo Costo: Generalmente son más económicos de producir y pueden ser integrados en un solo chip.

<br>
Bajo Consumo de Energía: Tienen un menor consumo energético, lo que es ideal para dispositivos móviles.

<br>
Alta Velocidad: Proporcionan altas velocidades de lectura, útiles para aplicaciones de video.

<br>Desventajas<br>
<br>Más Ruido: Pueden ser más susceptibles al ruido, especialmente en condiciones de poca luz.
<br>Calidad de Imagen Inferior: Aunque han mejorado, en algunas aplicaciones pueden ofrecer una calidad de imagen inferior comparada con los CCD.
<br>Aplicaciones<br>
<br>Usados en cámaras de teléfonos móviles, cámaras digitales de bajo consumo, y en la mayoría de los dispositivos de imagen de bajo costo.
<br><br>Otra comparativa de estas tecnologías es que los dispositivos CMOS tienen menos efecto Blooming o Efecto Halo a diferencia de los CCD, en cambio los CCD tienen menos efector Rolling Shutter<br>Blooming: es un efecto que ocurre principalmente en los sensores CCD. Aparece cuando un píxel se sobresatura al recibir más luz de la que puede manejar, lo que provoca que la carga de ese píxel "rebose" a los píxeles vecinos, generando halos brillantes o áreas sobreexpuestas en la imagen.<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=-DoTgQbALU0" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=-DoTgQbALU0" target="_blank">Ejemplo de comparación con <code></code></a>Blooming<br>Rolling Shutter: es un artefacto que aparece principalmente en sensores CMOS. En lugar de capturar toda la imagen a la vez, los sensores CMOS suelen leer cada fila de píxeles de forma secuencial. Esto crea un desfase temporal en la captura de la imagen, lo cual produce distorsiones en objetos en movimiento o en escenas con movimientos rápidos.<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=dNVtMmLlnoE" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=dNVtMmLlnoE" target="_blank">Explicación del <code></code></a>Rolling Shutter]]></description><link>1.-módulo-de-adquisición.html</link><guid isPermaLink="false">1. Módulo de Adquisición.md</guid><pubDate>Sun, 27 Oct 2024 10:04:11 GMT</pubDate><enclosure url="añadidos/captura-desde-2024-10-26-17-50-36.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="añadidos/captura-desde-2024-10-26-17-50-36.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2. La Imagen Digital]]></title><description><![CDATA[ 
 <br>En esta sección introduciremos la formación de la imagen y algunos procesos que se aplican a esta.<br><br>Ocurre un problema a la hora de hacer una fotografía y es la siguiente;<br>Lógicamente cuando hacemos una fotografía la hacemos de un objeto de 3 dimensiones con coordenadas , pero una imagen al fin y al cabo es una matriz de filas y columnas es decir se expresan en coordenadas , perder la componente  no es beneficioso.<br>¿Como podría obtener una posición concreta del objeto de la vida real, pero en la imagen?<br>Vamos a representar ambas posiciones en este sistema:<br><br><img alt="Pasted image 20241029175542.png" src="añadidos/pasted-image-20241029175542.png"><br>Supongamos que tenemos lo siguiente:<br>Estamos apuntando con nuestra cámara a un objeto del mundo real, la proyección de este pasa a través de la cámara.<br>Esta sería la distribución de los haces de luz:<br><img alt="Captura desde 2024-10-26 18-31-23.png" src="añadidos/captura-desde-2024-10-26-18-31-23.png"><br>Solo que en este caso  es ,  se considera despreciable por lo tanto,  es ,  es ,  es .<br>Por semejanza de triángulos se obtiene que  <br>Por antonomasia se obtiene que:<br><br><br>Muestreo:<br>
El muestreo es el proceso de convertir una imagen continua en una representación digital. En una imagen continua (como una fotografía), cada punto puede tomar valores precisos de intensidad o color en el espacio continuo. <br>Al digitalizar esta imagen para almacenamiento o procesamiento, se deben tomar muestras en intervalos regulares para obtener una cuadrícula de puntos o píxeles que representan la imagen.<br>
<br>Frecuencia de Muestreo: La frecuencia con la que se toman muestras afecta directamente la calidad de la imagen digital. De acuerdo con el Teorema de Nyquist, la frecuencia de muestreo debe ser al menos el doble de la frecuencia máxima de la imagen (detalle más pequeño) para evitar la pérdida de información.
<br>Resolución de la Imagen: La cantidad de muestras tomadas por unidad de área determina la resolución. Cuanto mayor sea la resolución, más fiel será la representación digital de la imagen continua original.
<br><img alt="Captura desde 2024-10-27 10-05-52.png" src="añadidos/captura-desde-2024-10-27-10-05-52.png"><br> Aliasing en Imágenes:<br>El aliasing ocurre cuando la frecuencia de muestreo es insuficiente para capturar todos los detalles de la imagen continua. Cuando esto sucede, los detalles de alta frecuencia (como líneas finas o texturas detalladas) aparecen distorsionados o producen patrones irreales y no deseados en la imagen digital. Algunos efectos comunes del aliasing incluyen:<br>
<br>Efecto Moiré: Un patrón de interferencia que aparece cuando se superponen patrones repetitivos, como líneas finas, y no se muestrean adecuadamente.
<br>Escalones y Bordes Dentados: Las líneas y curvas aparecen dentadas o en bloques debido a la baja resolución o al muestreo insuficiente, un fenómeno conocido como "aliasing espacial."
<br><img alt="Captura desde 2024-10-27 10-06-17.png" src="añadidos/captura-desde-2024-10-27-10-06-17.png"><br>Ejemplo de Aliasing<br>En una foto de una reja con líneas muy finas, si no se usa la frecuencia de muestreo adecuada, puede aparecer un patrón de interferencia o efecto Moiré en la imagen digital, donde algunas líneas parecen moverse o aparecer duplicadas.<br><img alt="Captura desde 2024-10-27 10-06-35.png" src="añadidos/captura-desde-2024-10-27-10-06-35.png"><br><br>Conversión de energía luminosa a voltaje:<br>
, es un modelo que describe la relación entre la señal de salida de un sensor de imagen (voltaje  ) y la intensidad de luz  que incide sobre el sensor. Cada componente de la ecuación tiene un papel específico:<br>
<br>
: Es el voltaje de salida o la señal generada por el sensor de imagen. Este valor está relacionado con la intensidad luminosa capturada en cada píxel y se convierte finalmente en el valor digital que se representa en la imagen.

<br>
: Es una constante de ganancia que ajusta el nivel de respuesta del sensor a la luz. Aumentar  implica que el sensor responderá más intensamente ante la misma cantidad de luz, amplificando la señal de salida.

<br>
: Es la intensidad de luz que incide sobre el píxel del sensor. Es la variable principal de entrada y representa la cantidad de energía luminosa que el sensor recibe.

<br>
: Es el parámetro de gamma o exponente no lineal que define cómo el sensor responde a diferentes niveles de luz. La gamma modifica la relación entre la luz que incide en el sensor y la señal de salida.

<br>Si , la relación es lineal: la salida del sensor es proporcional a la intensidad de luz .
<br>Si , la respuesta es sub-lineal, lo cual da más importancia a los tonos oscuros y aumenta el detalle en sombras.
<br>Si , la respuesta es sobre-lineal, lo cual aumenta el contraste en las áreas claras.

Este parámetro es importante para ajustar la respuesta del sensor y es común en el procesamiento de imágenes y la corrección gamma en pantallas.

<br>
 : Es un desplazamiento u offset que ajusta el nivel base de la señal, permitiendo que el sensor maneje niveles de luz de fondo o configuraciones de exposición específicas. Este término asegura que el voltaje de salida tenga una referencia o punto de partida adecuado en ausencia de luz (o en condiciones específicas de exposición).

<br><img alt="Captura desde 2024-10-27 10-49-30.png" src="añadidos/captura-desde-2024-10-27-10-49-30.png"><br>Cuantificación:<br>
Una vez que la luz ha sido convertida en voltaje, este valor analógico debe convertirse en un valor digital para que pueda procesarse y almacenarse en dispositivos electrónicos. Aquí es donde entra en juego la cuantificación mediante un convertidor analógico a digital (ADC):<br>
<br>Discretización del Voltaje: El rango de voltajes posibles en el sensor se divide en niveles discretos. Por ejemplo, en una imagen de 8 bits, hay 256 niveles posibles (0 a 255), donde cada nivel representa un rango específico de voltajes.<br>

<br>Asignación de Niveles de Intensidad: El ADC asigna a cada píxel un valor numérico correspondiente al nivel de voltaje que ha medido, generando así una representación digital de la imagen. En una imagen en escala de grises de 8 bits, un valor de 0 representaría negro absoluto (sin luz), y 255 representaría blanco absoluto (máxima intensidad de luz).<br>

<br>Profundidad de Bits: La cantidad de niveles de cuantificación está determinada por la profundidad de bits. A mayor profundidad de bits (por ejemplo, 10 bits o 12 bits), más niveles de intensidad se pueden representar, lo que permite una gama de tonos más amplia y una calidad de imagen más detallada. Sin embargo, esto también implica un mayor uso de almacenamiento y potencia de procesamiento.
<br><br>RGB:<br>
Se basa en representar cualquier color con los 3 colores primarios, en su gama de intensidades. Mejor la que el ordenador lo represente.<br><img alt="Pasted image 20241027105317.png" src="añadidos/pasted-image-20241027105317.png"><br>HSV:<br>
Se basa en función de 3 caracteristicas del color, Hue, Saturation y Value.<br>
Mejor interpretado por el ojo humano.<br><img alt="Pasted image 20241027105331.png" src="añadidos/pasted-image-20241027105331.png"><br><br>Un histograma de una imagen es un gráfico de frecuencias de cada uno de los píxeles de la imagen. <br><img alt="Pasted image 20241027105930.png" src="añadidos/pasted-image-20241027105930.png"><br>
<br>Análisis de Contraste:
<br>
<br>
Un histograma que abarca todo el rango de intensidades (desde el negro hasta el blanco) indica una buena distribución del contraste.

<br>
Un histograma concentrado en un extremo sugiere que la imagen podría estar subexpuesta (concentrado a la izquierda) o sobreexpuesta (concentrado a la derecha).

<br>
<br>Ecualización de Histogramas:
<br>
<br>
Este es un método para mejorar el contraste de una imagen al redistribuir los niveles de intensidad.

<br>
Al ecualizar un histograma, se busca que la distribución de los píxeles se extienda a través de todo el rango disponible, lo que resulta en una imagen con mayor contraste y más detalles.

<br>Clasificación de imágenes según su histograma<br>
<br>
Imagen Subexpuesta:<br>
Histograma concentrado a la izquierda, indicando que la mayoría de los píxeles son oscuros.

<br>
Imagen Sobreexpuesta:<br>
Histograma concentrado a la derecha, mostrando que la mayoría de los píxeles son claros.

<br>
Imagen Bien Expuesta:<br>
Histograma que abarca un rango amplio desde la izquierda hasta la derecha, indicando una buena distribución de tonos.

<br><img alt="Captura desde 2024-10-27 11-00-48.png" src="añadidos/captura-desde-2024-10-27-11-00-48.png">]]></description><link>2.-la-imagen-digital.html</link><guid isPermaLink="false">2. La Imagen Digital.md</guid><pubDate>Tue, 29 Oct 2024 18:02:37 GMT</pubDate><enclosure url="añadidos/pasted-image-20241029175542.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="añadidos/pasted-image-20241029175542.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[3. Procesado de la Imagen Digital]]></title><description><![CDATA[ 
 <br>En esta sección hablaremos de algunas técnicas de procesado de la imagen;<br>
en el Dominio Espacial en concreto puntual o con varias imágenes y basado en la vecindad.<br>Antes de comenzar me gustaría aclarar el concepto de Rango Dinámico de una imagen.<br>
<br>
En el contexto de la iluminación en fotografía, el rango dinámico se refiere a la capacidad de captar el detalle en las luces y sombras dentro de una misma imagen. 

<br>
Es la diferencia entre el nivel de luz más intenso y el más débil que una cámara puede capturar sin perder detalles.

<br><br>Normalización del Rango Dinámico:<br>Esta técnica puede ser interesante si se quiere: <br>
<br>Obtener un aumento del contraste para resaltar los detalles tanto de las sombras como de las luces, para tener una imagen más equlibrada y agradable.
<br>Mejora la claridad de la imagen.
<br>Puede mejorar la compatibilidad entre dispositivos con distinto rango dinámico.
<br>Trabajaremos 2 técnicas:<br>Normalización Lineal: Queremos reducir o expandir el rango dinámico.<br><br><br>Normalización de Media y Varianza:<br><br><br>Si fuera a la normal , sería:<br><br>Ajuste de gamma, contraste y brillo:<br>No hay mucho que comentar de esta técnica, mediante una operación píxel a píxel modifica la gamma, constraste y brillo.<br>¿Que es la gamma?<br>La gamma se refiere a un parámetro que describe la relación no lineal entre la intensidad de luz que se muestra en una pantalla (o se captura por un sensor) y la intensidad de luz real (o la señal de entrada). En otras palabras, la gamma define cómo se transforma la señal de voltaje en luminancia percibida.<br><br>
<br>Si , la relación es lineal: la salida del sensor es proporcional a la intensidad de luz .
<br>Si , la respuesta es sub-lineal, lo cual da más importancia a los tonos oscuros y aumenta el detalle en sombras.
<br>Si , la respuesta es sobre-lineal, lo cual aumenta el contraste en las áreas claras.
<br>Ecualización:<br>Esta técnica lo que busca hacer es que a partir del histograma de una imagen, se obtiene su histograma normalizado y acumulado y después aplicar una transformación de los píxeles de este histograma a la imagen real.<br>Esto se hace ya que este histograma esta mucho mejor repartido, se podría decir que sería el histograma de la imagen ideal a la original, ya que no estara ni sobreexpuesta ni subexpuesta.<br><img alt="Captura desde 2024-10-27 11-56-20.png" src="añadidos/captura-desde-2024-10-27-11-56-20.png"><br>LookUp Table:<br>Estas tablas básicamente, para aplicar una conversión de un píxel a otro dentro de una imagen, cada píxel tiene su transformación asociada, normalmente precalculada con el procesamiento que se le quiera aplicar.<br>Las lookup tables hacen mucho más eficiente el procesamiento de la imagen ya que no se tendrá que calcular la operación píxel a píxel de la imagen, ya que ya están precalculadas. <br>Un ejemplo muy sencillo es la inversión de una imagen a cada pixel se aplica , pero al estar precalculado es más eficiente.<br>Cambios de espacio de Color:<br>Los dos sistemas de color que hemos usado han sido por ahora RGB y HSV, es por eso que nos interesa poder cambiar entre estos, por ejemplo si queremos hacer una aplicación que aplique Chroma Key nos interesa cambiar al espacio de color HSV para tener el rango de colores del cual deseamos crear la máscara.<br>Balanceo de Color:<br>Trabajamos 2 técnicas: White Patch y Gray World.<br>La primera técnica White Patch, consiste en que el punto más blanco de una imagen debe ser el blanco, dependiendo de algunos parámetros lo que se suele hacer es buscar el punto más brillante de la imagen y reescalar esta para que el punto más brillante sea e blanco, otras versiones y para evitar blancos quemados, se usa la media se los p% puntos menos brillante.<br>En el caso del Gray World, lo que queremos obtener es neutralizar los dominantes de color, haciendo una imagen más neutra (más cercana al gris), de nuevo aplicando un reescalado.<br><br>
<br>
Suma: La suma de numerosas imágenes iguales con ruido hace que la imagen, gane nitidez.

<br>
Resta: Sirve principalmente para detectar movimientos en planos estáticos, si hacemos la diferencia de dos fotos de la misma escena y algo se ha movido, la diferencia saldrá todo en negro (0) menos la silueta del elemento distinto.

<br>
Multiplicación/División: pueden servir para reducir iluminaciones no deseadas dividiendo  imágenes mal iluminadas entre una imagen tipo que represente los tonos de luz que no queremos.

<br>
Operaciones Lógicas: Útiles para la composición de imágenes.

]]></description><link>3.-procesado-de-la-imagen-digital.html</link><guid isPermaLink="false">3. Procesado de la Imagen Digital.md</guid><pubDate>Sun, 27 Oct 2024 11:21:39 GMT</pubDate><enclosure url="añadidos/captura-desde-2024-10-27-11-56-20.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="añadidos/captura-desde-2024-10-27-11-56-20.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[4. Procesado Basado en la Vecindad]]></title><description><![CDATA[ 
 <br>En el procesamiento de imágenes, la vecindad de un píxel se refiere al conjunto de píxeles circundantes que rodean a un píxel central específico dentro de una imagen. <br>Estos píxeles vecinos son considerados cuando se aplican ciertos filtros y operaciones de procesamiento de imágenes que dependen del contexto local de cada píxel, como en operaciones de suavizado, detección de bordes y reducción de ruido.<br><br>La interpolación es una técnica para estimar valores de píxeles en posiciones que no coinciden con un píxel exacto de la imagen original. Es útil en operaciones como cambio de tamaño, rotación y distorsión de imágenes. <br>A continuación, se presentan tres métodos comunes: Vecino Más Cercano (NN), Bilineal y Bicúbica.<br>Vecino Más Cercano (Nearest Neighbor - NN):<br>Este es el método más simple y rápido, que asigna a un píxel el valor del píxel más cercano de la imagen original.<br>
<br>Método: Se busca el píxel en la imagen original más cercano a la posición en la imagen de salida y se asigna su valor al píxel de salida.
<br>Ventajas: Muy rápido y eficiente en cuanto a cómputo.
<br>Desventajas: Produce bordes "escalonados" o pixelados debido a la falta de suavidad entre píxeles.
<br>Aplicaciones: Útil en aplicaciones donde la velocidad es prioritaria, como en visualización rápida o en segmentación de imágenes.
<br><img alt="Captura desde 2024-10-27 21-21-48.png" src="añadidos/captura-desde-2024-10-27-21-21-48.png"><br>Interpolación Bilineal:<br>La interpolación bilineal considera no solo el píxel más cercano, sino también los píxeles vecinos inmediatos en direcciones horizontales y verticales. Calcula el valor del píxel de salida como una combinación ponderada de estos cuatro píxeles.<br>
<br>
Método:

<br>Encuentra los cuatro píxeles vecinos de la posición en la imagen original.
<br>Realiza interpolaciones lineales en las direcciones horizontal y vertical, promediando los valores según la distancia.


<br>
Ventajas: Transiciones más suaves entre píxeles, lo que reduce el efecto de pixelado.

<br>
Desventajas: Requiere más cálculos, lo cual puede aumentar el tiempo de procesamiento.

<br>
Aplicaciones: Ideal para redimensionamiento y rotación donde se busca mejorar la calidad visual sin mucha carga de procesamiento.

<br><img alt="Captura desde 2024-10-27 21-22-27.png" src="añadidos/captura-desde-2024-10-27-21-22-27.png"><br>Interpolación Bicúbica:<br>Este método avanzado usa 16 píxeles vecinos (4x4) y se basa en funciones cúbicas para transiciones más suaves y precisas.<br>
<br>
Método:

<br>Toma un bloque de 4x4 píxeles alrededor del píxel de interés.
<br>Calcula interpolación cúbica en cada dirección (horizontal y vertical) para una estimación precisa del píxel de salida.


<br>
Ventajas: Alta calidad visual, con transiciones muy suaves, ideal para imágenes con detalles finos.

<br>
Desventajas: Es el método más lento de los tres debido al número de cálculos necesarios, y puede introducir artefactos de suavizado.

<br>
Aplicaciones: Usado en aplicaciones donde se prioriza la calidad de imagen, como en edición de fotografías, impresión de alta calidad y gráficos computacionales.

<br><img alt="Captura desde 2024-10-27 21-23-02.png" src="añadidos/captura-desde-2024-10-27-21-23-02.png"><br><br><img alt="Captura desde 2024-10-27 21-23-25.png" src="añadidos/captura-desde-2024-10-27-21-23-25.png"><br>
<img alt="Captura desde 2024-10-27 21-23-51.png" src="añadidos/captura-desde-2024-10-27-21-23-51.png"><br><br>Correlación:<br><br>Básicamente por cada píxel de la imagen, extraemos una parcela del tamaño del filtro, multiplicamos en cada píxel de la parcela lo valores del filtro y suma esos valores al píxel en el que estamos trabajando.<br>Donde:<br>
 : es la imagen resultado.<br>
 : es la imagen de entrada.<br>
 : es el filtro.<br>La correlación se utiliza para buscar patrones y coincidencias.<br>Convolución:<br><br>Hacemos la misma operación que con la correlación solo que el filtro esta rotado 180º.<br>La convolución se suele aplicar  como desenfoque, detección de bordes y suavizado.<br>Vamos a ver algunos tipos de filtros lineales.<br>Filtro Paso Baja (Box Filter):<br><br>Bloquea las altas frecuencias para quedarnos con un resultado<br>
suavizado, con estructuras principales y sin detalles.<br>Filtro Paso Baja (Gaussian Filter):<br><br><br>Mismo objetivo que el anterior. Le da más importancia a los píxeles que comparten aristas con el seleccionado a estudio que a las diagonales. <br>Utiliza una distribución gaussiana con una constante  a elegir. <br>A mayor  la imagen obtenida es de más baja frecuencia.<br><img alt="Captura desde 2024-10-28 02-01-02.png" src="añadidos/captura-desde-2024-10-28-02-01-02.png"><br>Realce de Imagen (Unsharp Mask):<br>El Unsharp Mask (máscara de enfoque) es una técnica utilizada en el procesamiento de imágenes para mejorar la nitidez de una imagen. <br>A pesar de su nombre, el unsharp mask no hace que una imagen se vea menos nítida; en realidad, aumenta la claridad de los bordes y los detalles, creando la ilusión de que la imagen es más nítida.<br>Realiza la siguiente operación:<br><br>o también:<br><br>Donde:<br>
 : es la imagen de salida enfocada.<br>
 : es la imagen original.<br>
 : es un coeficiente que si es mayor que 1 se denomina high-boost-filtering.<br>
 : imagen desenfocada o suavizada.<br>A mayor  mayor enfoque.<br><img alt="Captura desde 2024-10-28 02-14-30.png" src="añadidos/captura-desde-2024-10-28-02-14-30.png"><br>Filtros Paso Alta (Derivadas de la imagen):<br>Las derivadas de una imagen son herramientas fundamentales en el procesamiento de imágenes, ya que permiten medir el cambio de intensidad en diferentes direcciones.<br>Esto es especialmente útil para detectar bordes, que son transiciones rápidas en la intensidad de píxeles.<br>La primera derivada indica el cambio de intensidad en el eje <br><br>
<br>Cero en las zonas constantes
<br>Distinto de cero en el escalón o comienzo/fin de rampas
<br>No cero durante la rampa
<br>Kernel para primera derivada:<br><br>La segunda derivada da información de la curvatura de la intensidad.<br><br>
<br>Cero en las zonas constantes
<br>Distinto de cero en las zonas de comienzo/fin de rampa
<br>Cero durante la rampa
<br>Kernel para segunda derivada:<br><br><img alt="Captura desde 2024-10-28 02-23-38.png" src="añadidos/captura-desde-2024-10-28-02-23-38.png"><br>Filtros Paso Alta (Filtro Sobel):<br>El filtro Sobel es una técnica comúnmente utilizada en el procesamiento de imágenes para la detección de bordes. <br>Se basa en calcular la derivada de la intensidad de la imagen en direcciones específicas (horizontal y vertical) para resaltar los cambios abruptos en la intensidad de los píxeles.<br>¿Como funciona?<br>El filtro Sobel utiliza dos matrices de convolución (kernels) que se aplican a la imagen para calcular la magnitud y la dirección del gradiente de la intensidad de la imagen.<br>Estas matrices están diseñadas para resaltar bordes y cambios de intensidad en las direcciones horizontal y vertical.<br><br><br>La magnitud del gradiente se puede calcular con:<br><br>Filtros Paso Alta (Laplaciano):<br>El operador Laplaciano es un filtro utilizado en el procesamiento de imágenes para detectar bordes y cambios abruptos en la intensidad de una imagen. <br>Se puede considerar un operador de paso alto porque realza las áreas donde hay cambios significativos, mientras que suprime las áreas donde la intensidad es constante.<br>Se representa con:<br><br>Sus kernels más representativos son:<br><br>Esta es otra representación que resalta los bordes de manera más intensa:<br><br>Realce de Imagen (Sharpenning):<br>El realce de imágenes o sharpening es un conjunto de técnicas utilizadas para mejorar la claridad y definición de los detalles en una imagen. <br>El objetivo principal del sharpening es resaltar los bordes y otros detalles significativos, haciendo que la imagen parezca más nítida.<br>Sus kernels más representativos:<br>Este kernel realza el píxel central mientras suprime los píxeles vecinos.<br><br>Este kernel es más agresivo en la detección de bordes.<br><br>Se deducen estos kernels de la siguiente interpretación del sharpenning:<br><br><br>En el filtrado no lineal, el valor de cada píxel se calcula mediante una operación no lineal basada en los valores de los píxeles vecinos, lo que permite resultados más adaptativos y robustos para ciertos tipos de ruido<br>Filtro por Mediana:<br>Sustituye el valor de cada píxel por la mediana de los valores de su vecindad.<br>Es especialmente útil para eliminar el ruido impulsivo o también llamado ruido de sal y pimienta.<br><img alt="Captura desde 2024-10-28 03-15-03.png" src="añadidos/captura-desde-2024-10-28-03-15-03.png"><br>Filtro Bilateral:<br>El filtro bilateral es un tipo de filtro no lineal que se utiliza en el procesamiento de imágenes para suavizar áreas sin afectar significativamente los bordes. Es muy popular para reducir el ruido en las imágenes, especialmente cuando se quiere preservar los detalles y bordes importantes.<br>A diferencia de otros filtros de suavizado, el filtro bilateral no solo considera la cercanía espacial de los píxeles, sino también la similitud de intensidad entre los píxeles vecinos, lo cual le permite diferenciar entre áreas con variación de intensidad gradual (como el interior de un objeto) y bordes definidos (donde hay un cambio brusco de intensidad).<br><br><img alt="Captura desde 2024-10-28 03-21-55.png" src="añadidos/captura-desde-2024-10-28-03-21-55.png"><br>Morfología (Operadores):<br>La erosión es un operador que reduce el tamaño de los objetos en la imagen. El objetivo es "erosionar" las fronteras de los objetos, lo que reduce el ruido y separa objetos que están ligeramente conectados.<br>
<br>
Definición: La erosión de una imagen  por un elemento estructurante  consiste en reducir las zonas de intensidad alta (blancas) en la imagen. En una imagen binaria, un píxel permanecerá en el valor alto (1 o blanco) solo si todos los píxeles de la vecindad definida por el elemento estructurante también son 1. Si algún píxel en la vecindad es 0 (negro), entonces el píxel central se convertirá en 0.

<br>
Efecto Visual: La erosión tiende a adelgazar las áreas blancas de la imagen y a eliminar pequeñas regiones y ruidos aislados.

<br>La dilatación es el operador opuesto a la erosión. En este caso, se expanden las áreas de intensidad alta (blancas) en la imagen. Este operador es útil para rellenar huecos en objetos, unir componentes vecinos y agrandar objetos en la imagen.<br>
<br>Definición: La dilatación de una imagen  por un elemento estructurante  consiste en expandir las zonas de intensidad alta (blancas). En una imagen binaria, un píxel se convierte en 1 si algún píxel en la vecindad definida por el elemento estructurante es 1.<br>

<br>Efecto Visual: La dilatación aumenta el tamaño de las áreas blancas y tiende a unir pequeñas regiones que están próximas, rellenando espacios entre objetos.
<br>En definitiva y lo importante que te tienes que quedar, es que en la dilatación te quedas con el píxel activo más grande y en la erosión con el más pequeño.<br><img alt="Captura desde 2024-10-28 03-28-51.png" src="añadidos/captura-desde-2024-10-28-03-28-51.png"><br>]]></description><link>4.-procesado-basado-en-la-vecindad.html</link><guid isPermaLink="false">4. Procesado Basado en la Vecindad.md</guid><pubDate>Sun, 17 Nov 2024 10:33:52 GMT</pubDate><enclosure url="añadidos/captura-desde-2024-10-27-21-21-48.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="añadidos/captura-desde-2024-10-27-21-21-48.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[6. Reconstrucción 3D]]></title><description><![CDATA[ 
 <br>En este apartado de la asignatura veremos algunas de las técnicas para recuperar la información 3D de las escena.<br>La idea principal o la primera idea sobre este tema es que, con una imagen sola no basta para poder obtener la perspectiva de una escena y menos poder reconstruirla en 3D, ya que una imagen es solo una cara 2D de la realidad.<br>Con dos podemos encontrar una correspondencia y a partir de varias de estas reconstruir la imagen, se puede entender también como triangular el punto 3D.<br><img alt="Pasted image 20241112113008.png" src="añadidos/pasted-image-20241112113008.png"><br><br>Esto en la práctica no siempre es eficiente ya que las rectas proyectivas de las vistas al punto 3D no siempre van a coincidir, debido al ruido debemos aproximar con la mayor precisión posible que las rectas proyectivas intersecten.<br><img alt="Pasted image 20241112113428.png" src="añadidos/pasted-image-20241112113428.png"><br><br>Epipolos:<br>
<br>Cada cámara tiene un epipolo, que es el punto en el plano de la imagen donde la línea que conecta los centros ópticos de las cámaras (la línea base) intersecta dicho plano de la imagen.
<br>Para dos cámaras, existe un epipolo en cada imagen: el epipolo derecho en la imagen de la cámara derecha y el epipolo izquierdo en la imagen de la cámara izquierda.
<br>Linea Epipolar:<br>
Debido a que ambas cámaras forman un plano, si queremos encontrar un punto de una imagen en la otra estará su correspondencia en la linea epipolar, esto reduce la complejidad de la búsqueda de  a .<br><img alt="Pasted image 20241112154018.png" src="añadidos/pasted-image-20241112154018.png"><br>Plano Epipolar:<br>
<br>El plano epipolar es el plano que contiene un punto en el espacio, así como los centros de proyección (los centros ópticos) de ambas cámaras.
<br>Este plano se proyecta como líneas epipolares en cada imagen.
<br><img alt="Pasted image 20241112153543.png" src="añadidos/pasted-image-20241112153543.png"><br><br>La visión estéreo es un método específico para la reconstrucción 3D que utiliza dos cámaras (o dos puntos de vista diferentes de una cámara) para simular cómo perciben la profundidad los humanos.<br>
<br>Par de Imágenes Estéreo: 
<br>
<br>La visión estéreo se basa en obtener un par de imágenes de la misma escena desde dos puntos de vista ligeramente diferentes.
<br>La disparidad entre los puntos correspondientes en cada imagen se puede usar para calcular la profundidad de cada punto en la escena.
<br>
<br>Mapeo de Disparidad:
<br>
<br>La disparidad es la diferencia en la posición de un punto en las dos imágenes. 
<br>A partir de la disparidad, y conociendo la distancia entre las cámaras (conocida como base estéreo), se puede calcular la profundidad de los puntos en la escena usando:
<br><img alt="Pasted image 20241112153318.png" src="añadidos/pasted-image-20241112153318.png"><br> donde: <br>
<br> es la profundidad, 
<br> es la distancia focal de la cámara,
<br> es la distancia entre las cámaras (base estéreo). 
<br>La disparidad es inversamente proporcional a la distancia, es decir a mayor distancia, menor disparidad y viceversa.<br><img alt="Pasted image 20241112154950.png" src="añadidos/pasted-image-20241112154950.png"><br>
<br>Correspondencia de Puntos: 
<br>
<br>
Uno de los principales desafíos en la visión estéreo es encontrar correspondencias exactas de puntos entre las dos imágenes.

<br>
Técnicas como el algoritmo de bloqueo, SAD (Suma de Diferencias Absolutas) o SIFT (Transformada Invariante de Escala) son algunas de las más comunes para este fin. 

<br>
<br>
Horópteros y Rango de Disparidad:
El horóptero es un concepto en visión estereoscópica que se refiere al conjunto de puntos en el espacio que, al proyectarse en los dos ojos o en un sistema de cámaras estéreo, generan imágenes en la misma posición relativa en ambas retinas (o sensores de las cámaras) sin ninguna disparidad. 
En otras palabras, los puntos en el horóptero tienen una disparidad cero.

<br>Puntos dentro del Rango de Disparidad:


<br>
Los puntos cercanos al horóptero, que presentan disparidades pequeñas (positivas o negativas) en función de la distancia con respecto al horóptero, caen dentro de este rango de disparidad.

<br>
Estos puntos están en un área conocida como el volumen de Panum en percepción humana, o simplemente el volumen de profundidad estéreo en visión artificial



<br>Puntos Fuera del Rango de Disparidad:


<br>Los puntos más allá del rango de disparidad máximo que el sistema puede procesar generan disparidades demasiado grandes para ser manejadas adecuadamente. Estos puntos aparecen desenfocados o sin correspondencia clara en las imágenes estéreo, dificultando la reconstrucción precisa de su profundidad.


<br>Relación con la Precisión en Estimación de Profundidad:


<br>
Dentro del rango de disparidad, el sistema estéreo tiene una precisión aceptable para estimar la profundidad de los puntos.

<br>
A medida que un punto se aleja del horóptero, su disparidad aumenta, y la precisión en la estimación de profundidad disminuye. Esto se debe a la relación inversa entre disparidad y profundidad: mayores disparidades se asocian con distancias más cercanas al sistema de cámaras, mientras que menores disparidades representan puntos más lejanos.
<img alt="Pasted image 20241112164926.png" src="añadidos/pasted-image-20241112164926.png">



<br><br>Ya sea por la luz o las condiciones que se tomaran las imágenes estas no podrán ser procesadas como si fuera en una situación ideal, también debido al ruido.<br>Es por eso que se hace un proceso de rectificación de la imagen para acercarla lo máximo posible a la situación ideal.<br>Se realiza el siguiente proceso:<br>El proceso de rectificación estéreo sigue generalmente estos pasos:<br>
<br>
Calibración de las Cámaras:

<br>Antes de rectificar, es necesario realizar una calibración de la cámara para obtener tanto los parámetros intrínsecos (focal, centro óptico) como los parámetros extrínsecos (rotación y traslación relativa entre las cámaras).
<br>Los parámetros extrínsecos son esenciales para entender cómo están orientadas y posicionadas las cámaras entre sí y en relación al mundo, ya que esto influye directamente en cómo se deben rectificar las imágenes.


<br>
Estimación de la Matriz de Rotación y Traslación:

<br>Con los parámetros de calibración, se calculan las matrices de rotación y traslación necesarias para alinear los planos de imagen de ambas cámaras de modo que sus ejes estén paralelos.
<br>Esta rotación reorienta las imágenes de manera que los pares de puntos de ambas cámaras se proyecten en las mismas líneas horizontales.

<img alt="Pasted image 20241112170655.png" src="añadidos/pasted-image-20241112170655.png">

<br>
Cálculo de las Nuevas Matrices de Proyección:

<br>Se generan nuevas matrices de proyección para ambas cámaras rectificadas, que aseguran que las imágenes estén alineadas y que los puntos homólogos se encuentren en la misma fila de cada imagen.
<br>Esto transforma las imágenes originales en imágenes "rectificadas", donde las correspondencias son más fáciles de encontrar.


<br>
Transformación de las Imágenes:

<br>Se aplica una transformación de perspectiva (usando las matrices de proyección calculadas) a las imágenes para crear las versiones rectificadas.
<br>Como resultado, los puntos correspondientes se encuentran ahora en la misma línea horizontal en ambas imágenes, facilitando el análisis estéreo.

<img alt="Pasted image 20241112171010.png" src="añadidos/pasted-image-20241112171010.png"> 
<img alt="Pasted image 20241112171710.png" src="añadidos/pasted-image-20241112171710.png">

<br>Uso del  para la búsqueda de la correspondencia:<br>También se usa para la correspondencia como antes mencione el SAD.<br><br>Lo que reduce la búsqueda de correspondencias a una dimensión.<br>
<br>Primero se procesa la imagen para normalizar el brillo y resaltar textura.
<br>Se aplica 
<br>Y por último se filtra para eliminar malas correspondencias.
<br><img alt="Pasted image 20241112172306.png" src="añadidos/pasted-image-20241112172306.png"><br><img alt="Pasted image 20241112172318.png" src="añadidos/pasted-image-20241112172318.png"><br><br><img alt="Pasted image 20241112172443.png" src="añadidos/pasted-image-20241112172443.png">]]></description><link>6.-reconstrucción-3d.html</link><guid isPermaLink="false">6. Reconstrucción 3D.md</guid><pubDate>Tue, 12 Nov 2024 16:24:44 GMT</pubDate><enclosure url="añadidos/pasted-image-20241112113008.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="añadidos/pasted-image-20241112113008.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[7. Otros Métodos de Reconstrucción 3D]]></title><description><![CDATA[ 
 <br>En esta sección abordaremos otros métodos de reconstrucción de la escena 3D.<br><br>La reconstrucción 3D mediante luz estructurada es una técnica ampliamente utilizada para obtener mapas de profundidad precisos y detallados de una escena. <br>A diferencia de la visión estéreo, que utiliza correspondencias entre dos o más imágenes capturadas desde diferentes puntos de vista, esta técnica proyecta patrones conocidos de luz sobre el objeto o la escena y analiza cómo estos patrones se deforman al interactuar con las superficies.<br><img alt="Pasted image 20241117111946.png" src="añadidos/pasted-image-20241117111946.png"><br> ¿Cómo Funciona la Luz Estructurada?<br>La técnica se basa en proyectar un patrón estructurado (como líneas, rejillas o patrones codificados) sobre el objeto y capturarlo con una o más cámaras. La deformación del patrón proyectado permite calcular la geometría 3D de la superficie.<br>Proceso Típico:<br>
<br>
Proyección del Patrón:

<br>Un proyector o láser emite un patrón estructurado sobre la escena.
<br>El patrón puede ser líneas paralelas, patrones de puntos, rejillas o incluso patrones codificados temporalmente.


<br>
Captura de Imágenes:

<br>Una cámara sincronizada con el proyector captura la escena iluminada.
<br>En configuraciones más avanzadas, varias cámaras pueden capturar el patrón desde diferentes ángulos para aumentar la precisión.


<br>
Cálculo de la Geometría 3D:

<br>Se analizan las deformaciones del patrón proyectado para determinar la profundidad o forma del objeto. Esto generalmente implica triangulación entre el proyector, la cámara y el punto observado.


<br>
Reconstrucción del Modelo 3D:

<br>A partir de los datos de profundidad calculados, se genera un modelo 3D de la escena.


<br><br><br><img alt="Pasted image 20241117113101.png" src="añadidos/pasted-image-20241117113101.png"><br>Ventajas:<br>
<br>
Alta Precisión:

<br>El código Gray minimiza errores de correspondencia al garantizar que cada píxel de la escena tenga un patrón único.


<br>
Tolerancia a Ruido:

<br>Los patrones codificados son robustos a pequeños errores en la captura debido a iluminación o propiedades superficiales.


<br>
Optimización del Tiempo de Decodificación:

<br>El código Gray es eficiente de calcular, ya que sus transiciones entre valores minimizan los errores por ruido.


<br>Inconvenientes:<br>
<br>
Sensibilidad a Superficies Reflectivas o Transparentes:

<br>Las superficies brillantes o transparentes pueden causar reflejos o dispersión, dificultando la captura precisa de los patrones proyectados.


<br>
Ambigüedades en Escenas Complejas:

<br>Para escenas con muchas oclusiones o geometrías complejas, es difícil garantizar correspondencias confiables entre cámara y proyector.


<br>
Resolución Limitada:

<br>La precisión depende de la resolución del proyector y la cámara. Para objetos muy pequeños o detalles finos, se requieren sistemas de alta resolución.


<br>
Tiempo de Procesamiento:

<br>Aunque el código Gray minimiza errores, proyectar múltiples patrones y procesar sus capturas lleva tiempo, lo que puede ser una limitación para aplicaciones en tiempo real.


<br><br>El método de luz estructurada con desplazamiento de fase es una técnica avanzada utilizada en la reconstrucción 3D, que se basa en proyectar patrones de franjas de luz sobre un objeto para capturar su forma tridimensional con alta precisión. <br>Este enfoque es especialmente útil para superficies con texturas complejas o reflectantes, donde otros métodos pueden fallar.<br><img alt="Pasted image 20241117115309.png" src="añadidos/pasted-image-20241117115309.png"><br>Solamente necesita 3 patrones.<br>Los patrones se calculan de la siguiente manera:<br> - Aquí  es la fase del patrón proyectado sobre la superficie. <br>
<br>Segundo patrón (desplazamiento de fase negativo):  -  representa el desplazamiento de fase, usualmente igual a  o  radianes. 
<br>Tercer patrón (desplazamiento de fase positivo): Parámetros en las fórmulas: 
<br>
<br>: Intensidad promedio o componente constante, que puede interpretarse como luz ambiente o base del patrón.
<br>: Amplitud del patrón sinusoidal, que determina el contraste de las franjas. 
<br>: Fase inicial del patrón, que contiene la información geométrica de la superficie proyectada. 
<br>: Desplazamiento de fase entre los patrones consecutivos, típicamente  o . 
<br>Proceso de cálculo de la fase:<br>
 Para reconstruir la geometría 3D, la fase envolvente  se calcula a partir de los tres patrones proyectados y capturados, usando las siguientes relaciones trigonométricas: <br>Fórmula de : <br>
<br>Numerador: Representa la diferencia entre las intensidades con fases desplazadas. 
<br>Denominador: Combinación ponderada de todas las intensidades capturadas.
<br>Ventajas del método <br>
<br>Precisión elevada: La utilización de múltiples patrones sinusoidales con desplazamientos de fase reduce el ruido y mejora la exactitud. 
<br>Fácil implementación: Generar y capturar patrones sinusoidales desplazados es eficiente tanto en hardware como en software. 
<br>Robustez frente a discontinuidades: El cálculo de la fase ayuda a reconstruir geometrías incluso en superficies complejas. 
<br> Inconvenientes: <br>
<br>Requiere sincronización precisa entre el proyector y la cámara para capturar las imágenes. 
<br>Sensibilidad a movimiento: Este método es sensible al movimiento del objeto o la cámara durante la captura de los patrones. 
<br>Desenvolvimiento de fase: Si la fase  tiene discontinuidades, se requiere un paso adicional de phase unwrapping para resolverlas.
<br><br>La Kinect es un dispositivo inicialmente diseñado por Microsoft para videojuegos, pero rápidamente encontró aplicaciones en campos como la reconstrucción 3D debido a su capacidad para capturar imágenes RGB y profundidades de manera simultánea. Este sistema combina tecnologías de luz estructurada e infrarroja, permitiendo obtener información tridimensional del entorno en tiempo real.<br><img alt="Pasted image 20241117155340.png" src="añadidos/pasted-image-20241117155340.png"><br> ¿Cómo funciona la Kinect para reconstrucción 3D?<br>La Kinect emplea un sistema basado en la emisión y detección de luz infrarroja:<br>
<br>
Proyección de un patrón infrarrojo:

<br>La Kinect utiliza un emisor de luz infrarroja que proyecta un patrón de puntos (generalmente una cuadrícula aleatoria o patrón pseudoestructurado) sobre la escena.
<br>Este patrón es diseñado para interactuar con las superficies del entorno y deformarse según la forma de los objetos.


<br>
Captura del patrón deformado:

<br>Una cámara infrarroja captura cómo se deforma el patrón en la escena. 
<br>La deformación del patrón proyectado depende de la distancia y las características de la superficie del objeto.


<br>
Cálculo de profundidad:

<br>Mediante triangulación, el sistema compara la posición de los puntos en el patrón proyectado con los detectados por la cámara infrarroja.
<br>Este cálculo proporciona un mapa de profundidad, donde cada píxel representa la distancia de un punto en la escena a la Kinect.


<br>
Imágenes RGB:

<br>Además del mapa de profundidad, la Kinect captura una imagen en color (RGB) que puede ser alineada con el mapa de profundidad.


<br>
Fusión de datos:

<br>Los datos de profundidad y color se combinan para generar una nube de puntos 3D que representa los objetos y el entorno capturado.<br>
<img alt="Pasted image 20241117155356.png" src="añadidos/pasted-image-20241117155356.png">


<br>Ventajas de la Kinect en reconstrucción 3D<br>
<br>Accesibilidad:

<br>La Kinect es relativamente económica comparada con otros sistemas de escaneo 3D.


<br>Tiempo real:

<br>Permite capturar datos 3D en tiempo real, lo cual es útil para aplicaciones interactivas.


<br>Facilidad de uso:

<br>Es sencilla de configurar y utilizar, con soporte para múltiples bibliotecas de software como OpenNI, PCL y Kinect SDK.


<br>Datos combinados:

<br>Proporciona tanto un mapa de profundidad como imágenes RGB, lo que facilita tareas como segmentación y texturización de los modelos 3D.


<br>Limitaciones del sistema Kinect<br>
<br>Resolución limitada:

<br>La calidad del mapa de profundidad no es comparable a sistemas láser o de fotogrametría de alta precisión.


<br>Rango operativo:

<br>Funciona mejor en rangos de distancia entre 0.5 y 4 metros.


<br>Problemas con superficies específicas:

<br>Dificultad para capturar superficies reflectantes, transparentes o extremadamente oscuras.


<br>Sensibilidad a la iluminación:

<br>Aunque funciona en entornos oscuros gracias al infrarrojo, puede fallar en condiciones de iluminación intensa o cuando otras fuentes infrarrojas interfieren.


<br><br>La reconstrucción 3D a partir de siluetas es una técnica utilizada para crear modelos tridimensionales a partir de múltiples imágenes tomadas desde diferentes ángulos, aprovechando las proyecciones del objeto en cada imagen para determinar su forma aproximada. Esta técnica puede implementarse utilizando métodos como Voxel Set y Octree.<br>Concepto básico:<br>
<br>
Definición de silueta:

<br>La silueta de un objeto es su proyección binaria sobre una imagen (región ocupada por el objeto frente a un fondo claro o negro).


<br>
Espacio visual:

<br>Cada silueta proyectada define un cono visual en el espacio tridimensional, con el vértice en la posición de la cámara y la base definida por la silueta en el plano de imagen.


<br>
Volumen visual (Visual Hull):

<br>La intersección de los conos visuales generados por todas las siluetas da lugar al Visual Hull, que es una aproximación volumétrica del objeto original.


<br>
Suposiciones:

<br>El objeto es opaco.
<br>Las cámaras están calibradas (se conocen las posiciones y parámetros de las cámaras).


<br><img alt="Pasted image 20241117155846.png" src="añadidos/pasted-image-20241117155846.png"><br><br>Un voxel (volumetric pixel) es una unidad cúbica que representa una porción del espacio 3D, similar a cómo un píxel representa un punto en 2D.<br><img alt="Pasted image 20241117160923.png" src="añadidos/pasted-image-20241117160923.png"><br>Método de reconstrucción:<br>
<br>
Definición del espacio 3D:

<br>El espacio tridimensional se divide en una cuadrícula uniforme de voxeles.


<br>
Proyección de los voxeles:

<br>Cada voxel se proyecta en todas las imágenes disponibles.
<br>Si el voxel cae dentro de la silueta en todas las imágenes, se clasifica como interior; de lo contrario, se clasifica como exterior.


<br>
Resultado:

<br>La reconstrucción final es un volumen discreto compuesto por los voxeles clasificados como interiores.


<br><img alt="Pasted image 20241117161118.png" src="añadidos/pasted-image-20241117161118.png"><br>Ventajas:<br>
<br>Fácil de implementar.
<br>Buena aproximación para objetos con formas regulares.
<br>Inconvenientes:<br>
<br>Consumo elevado de memoria para representaciones de alta resolución.
<br>La precisión depende del tamaño de los voxeles: más pequeños significan mayor precisión pero mayor costo computacional.
<br><br>
<br>Un Octree es una estructura jerárquica de datos que subdivide el espacio 3D en ocho partes (octantes) de manera recursiva.
<br>Cada nodo del Octree representa un volumen cúbico que puede subdividirse en octantes más pequeños.
<br><img alt="Pasted image 20241117161056.png" src="añadidos/pasted-image-20241117161056.png"><br>Método de reconstrucción:<br>
<br>
Inicialización del espacio:

<br>Todo el espacio 3D se representa como un único nodo cúbico.


<br>
Proyección y subdivisión:

<br>Si el nodo cúbico intersecta con el Visual Hull, se subdivide en ocho subceldas (octantes).
<br>Este proceso continúa hasta alcanzar una resolución deseada o hasta que una celda se clasifique completamente como interior o exterior.


<br>
Resultado:

<br>El modelo final es una aproximación jerárquica del objeto, donde las regiones interiores y exteriores están representadas con diferentes niveles de detalle.


<br>Ventajas:<br>
<br>Eficiente en memoria: solo subdivide las regiones necesarias.
<br>Permite un balance entre precisión y rendimiento.
<br>Limitaciones:<br>
<br>Más complejo de implementar que el método de Voxel Set.
<br>Puede perder precisión en las regiones con bordes complejos si no se subdivide adecuadamente.
<br><br><br>En la reconstrucción 3D mediante el movimiento de la cámara, las matrices fundamentales, esenciales y de cámara son esenciales para entender cómo se transforman las imágenes 2D para estimar la geometría tridimensional de la escena. Estas matrices están relacionadas con los parámetros intrínsecos y extrínsecos de las cámaras y son fundamentales en procesos como la triangulación y la correspondencia de puntos.<br><img alt="Pasted image 20241117163935.png" src="añadidos/pasted-image-20241117163935.png"><br><br>La matriz fundamental relaciona las coordenadas de un punto en una imagen con su punto correspondiente en otra imagen.<br>Definición:<br>
La matriz fundamental  cumple la siguiente ecuación:<br><br>Donde:<br>
<br> y  son los puntos correspondientes en las dos imágenes.
<br> es una matriz de  que describe la relación entre estos puntos.
<br>Propiedades:<br>
<br>Tiene rango 2.
<br>No depende de los parámetros intrínsecos, pero sí de la configuración relativa (rotación y traslación) entre las cámaras.
<br>Relación con la Matriz Esencial:<br>Si las cámaras están calibradas, la matriz fundamental  se relaciona con la matriz esencial  mediante los parámetros intrínsecos de las cámaras:<br><br><br>La matriz esencial también relaciona puntos correspondientes entre imágenes, pero se usa cuando las cámaras están calibradas y conocemos los parámetros intrínsecos.<br>Definición:<br>
La matriz esencial  cumple la misma ecuación que la matriz fundamental:<br><br>Pero en este caso,  y  son los puntos en coordenadas normalizadas. La matriz esencial se calcula como:<br><br>Donde:<br>
<br> es la matriz antisimétrica de la traslación .
<br> es la matriz de rotación entre las cámaras.
<br>Propiedades:<br>
<br>Tiene rango 2.
<br>Depende de los parámetros extrínsecos (rotación y traslación) entre las cámaras.
<br><br>La matriz de cámara o matriz intrínseca describe cómo se proyectan las coordenadas 3D en las coordenadas 2D de la imagen.<br>Definición:<br>
La matriz intrínseca  tiene la forma:<br><br>Donde:<br>
<br>,  son las distancias focales en los ejes  y .
<br> es el centro óptico de la imagen.
<br><br>Proceso de Reconstrucción 3D con el Movimiento de la Cámara:<br>
<br>
Captura de Imágenes: Se toman dos imágenes desde diferentes posiciones de la cámara.

<br>
Correspondencia de Puntos: Se identifican puntos correspondientes entre las dos imágenes utilizando algoritmos como SIFT o SURF.

<br>
Cálculo de la Matriz Fundamental: Se obtiene  a partir de los puntos correspondientes. Si las cámaras están calibradas, se usa la matriz esencial .

<br>
Estimación de los Parámetros Extrínsecos: A partir de , se calcula la rotación  y la traslación .

<br>
Triangulación: Utilizando ,  y las matrices de cámara, se calculan las coordenadas 3D de los puntos.

<br><br><br>
<br>Matriz Fundamental (): Relaciona los puntos correspondientes en imágenes no calibradas.
<br>Matriz Esencial (): Relaciona los puntos correspondientes en imágenes calibradas y depende de  y .
<br>Matriz de Cámara (): Contiene los parámetros intrínsecos de la cámara.
<br>Estas matrices trabajan juntas para calcular la geometría 3D de la escena y posicionar la cámara en el espacio tridimensional.<br><br>El método Simultaneous Localization and Mapping (SLAM) es un enfoque crucial en robótica y visión computacional para la reconstrucción 3D y la navegación en entornos desconocidos. Este método permite que un sistema construya un mapa del entorno mientras simultáneamente estima su propia posición dentro de dicho mapa.<br><br>El proceso SLAM combina datos de múltiples sensores, como cámaras, lidar o IMUs (Unidades de Medición Inercial), para realizar dos tareas fundamentales:<br>
<br>Localización: Determinar la posición y orientación del sistema dentro del entorno.
<br>Mapeo: Construir un mapa del entorno que pueda ser utilizado para navegación o reconstrucción.
<br><img alt="Pasted image 20241117165509.png" src="añadidos/pasted-image-20241117165509.png"><br><br>
<br>
Entrada de Datos:

<br>Sensores de percepción: Capturan la información del entorno (imágenes, nubes de puntos, etc.).
<br>Sensores de movimiento: Proveen datos sobre la posición y orientación del sistema (giroscopios, acelerómetros).


<br>
Procesamiento de Datos:

<br>Estimación de la posición: Algoritmos como filtro de partículas o filtros de Kalman extendidos (EKF) son utilizados para predecir la posición.
<br>Correspondencias: Identificar elementos clave (features) del entorno que puedan ser rastreados en múltiples capturas.


<br>
Construcción del Mapa:

<br>Generación de representaciones del entorno, como mapas 2D, modelos 3D o gráficas de poses (pose graphs).


<br>
Corrección de Errores:

<br>Técnicas como loop closure detectan cuando el sistema regresa a un área conocida, permitiendo corregir errores acumulados.


<br>Ventajas del SLAM:<br>
<br>
Autonomía en entornos desconocidos:

<br>Fundamental para robots autónomos, drones y vehículos sin conductor.


<br>
Alta precisión:

<br>Los métodos avanzados de SLAM ofrecen mapeos detallados y localización precisa.


<br>
Adaptabilidad:

<br>Funciona en entornos dinámicos y bajo diversas condiciones.


<br>Inconvenientes del SLAM:<br>
<br>
Consumo computacional:

<br>Requiere hardware potente para procesar datos en tiempo real.


<br>
Errores acumulativos:

<br>La estimación de la posición puede acumular errores si no se corrigen adecuadamente.


<br>
Sensibilidad a condiciones del entorno:

<br>La iluminación, texturas pobres o superficies reflectantes pueden afectar la calidad de los mapas en Visual SLAM.


]]></description><link>7.-otros-métodos-de-reconstrucción-3d.html</link><guid isPermaLink="false">7. Otros Métodos de Reconstrucción 3D.md</guid><pubDate>Sun, 17 Nov 2024 15:55:50 GMT</pubDate><enclosure url="añadidos/pasted-image-20241117111946.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="añadidos/pasted-image-20241117111946.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[index]]></title><description><![CDATA[ 
 <br><br><a data-href="1. Módulo de Adquisición" href="1.-módulo-de-adquisición.html" class="internal-link" target="_self" rel="noopener nofollow">1. Módulo de Adquisición</a><br>
<a data-href="2. La Imagen Digital" href="2.-la-imagen-digital.html" class="internal-link" target="_self" rel="noopener nofollow">2. La Imagen Digital</a><br>
<a data-href="3. Procesado de la Imagen Digital" href="3.-procesado-de-la-imagen-digital.html" class="internal-link" target="_self" rel="noopener nofollow">3. Procesado de la Imagen Digital</a><br>
<a data-href="4. Procesado Basado en la Vecindad" href="4.-procesado-basado-en-la-vecindad.html" class="internal-link" target="_self" rel="noopener nofollow">4. Procesado Basado en la Vecindad</a><br><br><a data-href="5. Modelo Proyectivo" href="5.-modelo-proyectivo.html" class="internal-link" target="_self" rel="noopener nofollow">5. Modelo Proyectivo</a><br>
<a data-href="6. Reconstrucción 3D" href="6.-reconstrucción-3d.html" class="internal-link" target="_self" rel="noopener nofollow">6. Reconstrucción 3D</a><br>
<a data-href="7. Otros Métodos de Reconstrucción 3D" href="7.-otros-métodos-de-reconstrucción-3d.html" class="internal-link" target="_self" rel="noopener nofollow">7. Otros Métodos de Reconstrucción 3D</a>]]></description><link>index.html</link><guid isPermaLink="false">index.md</guid><pubDate>Sun, 17 Nov 2024 15:19:55 GMT</pubDate></item></channel></rss>