<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Apuntes de FSIV]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib/media/favicon.png</url><title>Apuntes de FSIV</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Tue, 12 Nov 2024 16:40:08 GMT</lastBuildDate><atom:link href="lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Tue, 12 Nov 2024 16:40:02 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[1. Módulo de Adquisición]]></title><description><![CDATA[ 
 <br>En esta sección vamos a conocer los distintos elementos hardware que integran el módulo de adquisición SIVA<br><br>La luz es una&nbsp;onda electromagnética capaz de ser percibida por el ojo humano y cuya frecuencia determina su color.<br>Espectro Visible:<br><img alt="Captura desde 2024-10-26 17-50-36.png" src="añadidos/captura-desde-2024-10-26-17-50-36.png"><br><br>Para poder formar una imagen digital capturada del mundo real, vamos a necesitar los siguientes elementos fundamentales:<br>
<br>Una fuente de Luz: 
<br>Un objeto que reciba la luz, la refleje : 
<br>Un sensor o receptor de la luz: 
<br>Siendo la luz total dentro de un periodo de tiempo acotado:<br><br><img alt="Captura desde 2024-10-26 18-00-57.png" src="añadidos/captura-desde-2024-10-26-18-00-57.png"><br><br>El comportamiento de la luz y la captura de imágenes esta directamente relacionada con las propiedades de los objetos en estas.<br>Propiedades:<br>
<br>Absorbentes: Selectividad del espectro, definen el color del objeto.
<br>Reflexivas: Materiales que reflejan parte de la luz que reciben (Especulares o Difusos).
<br>Transmitivas: El paso de la luz a través de ellos (Ópacos, Translucidos y Transparentes).
<br><img alt="Captura desde 2024-10-26 18-06-08.png" src="añadidos/captura-desde-2024-10-26-18-06-08.png"><br><br>Dentro de la captura de imágenes nos pueden interesar distintos tipos de iluminación en función de los parámetros de la escena.<br>Estas pueden ser Natural, Incandescente, Fluorescentes, Estroboscópica, Láser, LED ...<br>Además de las fuentes de luz se pueden aplicar distintas técnicas.<br>
<br>Direccional: Lateral, Coaxial, Campo Oscuro
<br>Difusa
<br>A contraluz
<br>Iluminación Estructurada
<br><br><br>El modelo de lente fina es una simplificación idealizada del comportamiento óptico de una lente. Este modelo asume que la lente tiene un espesor despreciable y que los rayos de luz pasan a través de un solo plano central.<br>Hay que aclarar que se hace uso de una lente convergente.<br>Se rige por esta fórmula:<br><br>Donde:<br>
<br> es la distancia focal
<br> es la distancia real al objeto
<br> es la distancia en la imagen
<br><img alt="Captura desde 2024-10-26 18-31-23.png" src="añadidos/captura-desde-2024-10-26-18-31-23.png"><br>Vamos a denotan  como la proyección del objeto del mundo real al que le estamos echando la foto y  a la proyección de la imagen. <br>(También hay que aclarar que esta definición es para que yo la entienda mejor puedo no estar bien expresado).<br>A partir de aquí vamos a hablar de algunos parámetros de lente:<br>Distancia focal :<br>La distancia focal es la distancia entre el centro de la lente y el punto donde los rayos de luz paralelos al eje óptico se enfocan (el punto focal).<br>Cuanto menor sea la distancia focal mayor dispersión de la imagen habrá, por lo tanto más borrosa.<br><img alt="Captura desde 2024-10-26 18-41-55.png" src="añadidos/captura-desde-2024-10-26-18-41-55.png"><br>Coeficiente de Magnificación :<br>La magnificación o aumento en una lente fina describe cómo cambia el tamaño del objeto al formar su imagen.<br><br>
<br>Si  entonces la imagen ha aumentado en comparación al objeto del mundo real.
<br>Si  entonces la imagen a disminuido en comparación al objeto del mundo real.
<br>Aquí pondré la deducción matemática de la distancia focal a partir del aumento:<br>Tomamos de referencia esta imagen: <br><img alt="Captura desde 2024-10-26 18-31-23.png" src="añadidos/captura-desde-2024-10-26-18-31-23.png"><br>A partir de los cuales se obtienen 2 triángulos semejantes (Ambos angulos son el mismo).<br><img alt="Captura desde 2024-10-26 20-09-08.png" src="añadidos/captura-desde-2024-10-26-20-09-08.png"><br>Aplicando reglas trigonométricas se obtiene que:<br><br><br>Igualamos:<br><br>Despejamos en función de :<br><br><br> se considera despreciable.<br>Diafragma :<br>
<br>
El diafragma es un mecanismo ajustable dentro del lente de una cámara que regula la cantidad de luz que pasa a través de él. Funciona como un orificio circular cuya apertura puede cambiarse para controlar la intensidad de luz que alcanza el sensor o la película. 

<br>
La apertura se mide en términos del número .

<br>
Control de Luz: Al abrir más el diafragma, entra más luz, y al cerrarlo, entra menos luz.

<br>
Profundidad de Campo: Una apertura mayor (número  menor) genera una profundidad de campo más reducida, enfocando solo una parte de la escena, mientras que una apertura pequeña (número  alto) aumenta la profundidad de campo, manteniendo más elementos de la imagen en foco.

<br>Número  :<br>El número , también llamado relación focal o apertura relativa, es una medida de la apertura del diafragma de la lente, representada con la letra f seguida de un número (como f/2.8, f/5.6, etc.). Este número indica la relación entre la distancia focal del lente y el diámetro efectivo de la apertura:<br>Donde:<br>
<br> es la distancia focal.
<br> es el diámetro de la apertura efectiva del diafragma.
<br>Hay que detallar que  sigue una escala geométrica: <br>Aquí voy a poner algunas aplicaciones de distintos usos de números :<br>
<br>
Retratos: Un número  bajo (f/1.4 o f/2.8) es ideal para retratos, ya que produce un desenfoque de fondo que hace que el sujeto destaque.

<br>
Paisajes: Un número  alto (f/8 o superior) es útil en fotografía de paisajes, donde se necesita una profundidad de campo extensa para mantener todos los elementos en foco.

<br>
Fotografía Nocturna: Usar un número  bajo permite captar más luz, ideal en condiciones de baja iluminación.

<br><img alt="Pasted image 20241026191840.png" src="añadidos/pasted-image-20241026191840.png"><br><br>Dentro de los sensores de cámara podemos distinguir 2 tecnologías muy importantes:<br>
<br>
CCD

<br>
CMOS
CCD (Charge-Coupled Device)

<br>
Funcionamiento: Los sensores CCD convierten la luz en carga eléctrica, que se transfiere a través de la matriz de píxeles a un convertidor analógico-digital (ADC) para formar una imagen.

<br>
Calidad de Imagen: Generalmente, los sensores CCD ofrecen una mejor calidad de imagen en términos de ruido, sensibilidad a la luz y rango dinámico. Esto se debe a su diseño que minimiza el ruido electrónico.

<br>
Sensibilidad a la Luz: Tienen una alta sensibilidad, lo que los hace ideales para fotografía en condiciones de poca luz.

<br>
Consumo de Energía: Consume más energía que los sensores CMOS, lo que puede ser un factor limitante en aplicaciones portátiles.

<br><img alt="Captura desde 2024-10-27 09-07-28.png" src="añadidos/captura-desde-2024-10-27-09-07-28.png"><br> Ventajas<br>
<br>Menos Ruido: Producen imágenes con menos ruido, especialmente en situaciones de baja iluminación.
<br>Alta Calidad de Imagen: Proporcionan una mejor uniformidad en la respuesta del sensor, lo que se traduce en una mayor calidad de imagen.
<br>Desventajas<br>
<br>Costo: Generalmente más caros de producir que los sensores CMOS.
<br>Velocidad: Tienen velocidades de lectura más lentas, lo que puede ser una desventaja en aplicaciones de video de alta velocidad o en captura de imágenes en ráfaga.
<br>Aplicaciones<br>
<br>Usados en cámaras profesionales, telescopios, y aplicaciones científicas donde se requiere alta calidad de imagen.
<br>CMOS (Complementary Metal-Oxide-Semiconductor)<br>Características<br>
<br>
Funcionamiento: Los sensores CMOS utilizan transistores individuales para cada píxel, lo que permite que cada píxel convierta la luz en carga eléctrica y la procese directamente.

<br>
Calidad de Imagen: Aunque han mejorado significativamente en calidad de imagen, los sensores CMOS pueden tener más ruido que los CCD en condiciones de poca luz, aunque esto ha mejorado en modelos recientes.

<br>
Consumo de Energía: Más eficientes en términos de consumo de energía, lo que los hace adecuados para dispositivos portátiles.

<br>
Velocidad: Ofrecen velocidades de lectura más rápidas, lo que es beneficioso para video de alta velocidad y captura de imágenes en ráfaga.

<br><img alt="Captura desde 2024-10-27 09-12-27.png" src="añadidos/captura-desde-2024-10-27-09-12-27.png"><br>Ventajas<br>
<br>
Bajo Costo: Generalmente son más económicos de producir y pueden ser integrados en un solo chip.

<br>
Bajo Consumo de Energía: Tienen un menor consumo energético, lo que es ideal para dispositivos móviles.

<br>
Alta Velocidad: Proporcionan altas velocidades de lectura, útiles para aplicaciones de video.

<br>Desventajas<br>
<br>Más Ruido: Pueden ser más susceptibles al ruido, especialmente en condiciones de poca luz.
<br>Calidad de Imagen Inferior: Aunque han mejorado, en algunas aplicaciones pueden ofrecer una calidad de imagen inferior comparada con los CCD.
<br>Aplicaciones<br>
<br>Usados en cámaras de teléfonos móviles, cámaras digitales de bajo consumo, y en la mayoría de los dispositivos de imagen de bajo costo.
<br><br>Otra comparativa de estas tecnologías es que los dispositivos CMOS tienen menos efecto Blooming o Efecto Halo a diferencia de los CCD, en cambio los CCD tienen menos efector Rolling Shutter<br>Blooming: es un efecto que ocurre principalmente en los sensores CCD. Aparece cuando un píxel se sobresatura al recibir más luz de la que puede manejar, lo que provoca que la carga de ese píxel "rebose" a los píxeles vecinos, generando halos brillantes o áreas sobreexpuestas en la imagen.<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=-DoTgQbALU0" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=-DoTgQbALU0" target="_blank">Ejemplo de comparación con <code></code></a>Blooming<br>Rolling Shutter: es un artefacto que aparece principalmente en sensores CMOS. En lugar de capturar toda la imagen a la vez, los sensores CMOS suelen leer cada fila de píxeles de forma secuencial. Esto crea un desfase temporal en la captura de la imagen, lo cual produce distorsiones en objetos en movimiento o en escenas con movimientos rápidos.<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=dNVtMmLlnoE" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=dNVtMmLlnoE" target="_blank">Explicación del <code></code></a>Rolling Shutter]]></description><link>1.-módulo-de-adquisición.html</link><guid isPermaLink="false">1. Módulo de Adquisición.md</guid><pubDate>Sun, 27 Oct 2024 10:04:11 GMT</pubDate><enclosure url="añadidos/captura-desde-2024-10-26-17-50-36.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;añadidos/captura-desde-2024-10-26-17-50-36.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2. La Imagen Digital]]></title><description><![CDATA[ 
 <br>En esta sección introduciremos la formación de la imagen y algunos procesos que se aplican a esta.<br><br>Ocurre un problema a la hora de hacer una fotografía y es la siguiente;<br>Lógicamente cuando hacemos una fotografía la hacemos de un objeto de 3 dimensiones con coordenadas , pero una imagen al fin y al cabo es una matriz de filas y columnas es decir se expresan en coordenadas , perder la componente  no es beneficioso.<br>¿Como podría obtener una posición concreta del objeto de la vida real, pero en la imagen?<br>Vamos a representar ambas posiciones en este sistema:<br><br><img alt="Pasted image 20241029175542.png" src="añadidos/pasted-image-20241029175542.png"><br>Supongamos que tenemos lo siguiente:<br>Estamos apuntando con nuestra cámara a un objeto del mundo real, la proyección de este pasa a través de la cámara.<br>Esta sería la distribución de los haces de luz:<br><img alt="Captura desde 2024-10-26 18-31-23.png" src="añadidos/captura-desde-2024-10-26-18-31-23.png"><br>Solo que en este caso  es ,  se considera despreciable por lo tanto,  es ,  es ,  es .<br>Por semejanza de triángulos se obtiene que  <br>Por antonomasia se obtiene que:<br><br><br>Muestreo:<br>
El muestreo es el proceso de convertir una imagen continua en una representación digital. En una imagen continua (como una fotografía), cada punto puede tomar valores precisos de intensidad o color en el espacio continuo. <br>Al digitalizar esta imagen para almacenamiento o procesamiento, se deben tomar muestras en intervalos regulares para obtener una cuadrícula de puntos o píxeles que representan la imagen.<br>
<br>Frecuencia de Muestreo: La frecuencia con la que se toman muestras afecta directamente la calidad de la imagen digital. De acuerdo con el Teorema de Nyquist, la frecuencia de muestreo debe ser al menos el doble de la frecuencia máxima de la imagen (detalle más pequeño) para evitar la pérdida de información.
<br>Resolución de la Imagen: La cantidad de muestras tomadas por unidad de área determina la resolución. Cuanto mayor sea la resolución, más fiel será la representación digital de la imagen continua original.
<br><img alt="Captura desde 2024-10-27 10-05-52.png" src="añadidos/captura-desde-2024-10-27-10-05-52.png"><br> Aliasing en Imágenes:<br>El aliasing ocurre cuando la frecuencia de muestreo es insuficiente para capturar todos los detalles de la imagen continua. Cuando esto sucede, los detalles de alta frecuencia (como líneas finas o texturas detalladas) aparecen distorsionados o producen patrones irreales y no deseados en la imagen digital. Algunos efectos comunes del aliasing incluyen:<br>
<br>Efecto Moiré: Un patrón de interferencia que aparece cuando se superponen patrones repetitivos, como líneas finas, y no se muestrean adecuadamente.
<br>Escalones y Bordes Dentados: Las líneas y curvas aparecen dentadas o en bloques debido a la baja resolución o al muestreo insuficiente, un fenómeno conocido como "aliasing espacial."
<br><img alt="Captura desde 2024-10-27 10-06-17.png" src="añadidos/captura-desde-2024-10-27-10-06-17.png"><br>Ejemplo de Aliasing<br>En una foto de una reja con líneas muy finas, si no se usa la frecuencia de muestreo adecuada, puede aparecer un patrón de interferencia o efecto Moiré en la imagen digital, donde algunas líneas parecen moverse o aparecer duplicadas.<br><img alt="Captura desde 2024-10-27 10-06-35.png" src="añadidos/captura-desde-2024-10-27-10-06-35.png"><br><br>Conversión de energía luminosa a voltaje:<br>
, es un modelo que describe la relación entre la señal de salida de un sensor de imagen (voltaje  ) y la intensidad de luz  que incide sobre el sensor. Cada componente de la ecuación tiene un papel específico:<br>
<br>
: Es el voltaje de salida o la señal generada por el sensor de imagen. Este valor está relacionado con la intensidad luminosa capturada en cada píxel y se convierte finalmente en el valor digital que se representa en la imagen.

<br>
: Es una constante de ganancia que ajusta el nivel de respuesta del sensor a la luz. Aumentar  implica que el sensor responderá más intensamente ante la misma cantidad de luz, amplificando la señal de salida.

<br>
: Es la intensidad de luz que incide sobre el píxel del sensor. Es la variable principal de entrada y representa la cantidad de energía luminosa que el sensor recibe.

<br>
: Es el parámetro de gamma o exponente no lineal que define cómo el sensor responde a diferentes niveles de luz. La gamma modifica la relación entre la luz que incide en el sensor y la señal de salida.

<br>Si , la relación es lineal: la salida del sensor es proporcional a la intensidad de luz .
<br>Si , la respuesta es sub-lineal, lo cual da más importancia a los tonos oscuros y aumenta el detalle en sombras.
<br>Si , la respuesta es sobre-lineal, lo cual aumenta el contraste en las áreas claras.

Este parámetro es importante para ajustar la respuesta del sensor y es común en el procesamiento de imágenes y la corrección gamma en pantallas.

<br>
 : Es un desplazamiento u offset que ajusta el nivel base de la señal, permitiendo que el sensor maneje niveles de luz de fondo o configuraciones de exposición específicas. Este término asegura que el voltaje de salida tenga una referencia o punto de partida adecuado en ausencia de luz (o en condiciones específicas de exposición).

<br><img alt="Captura desde 2024-10-27 10-49-30.png" src="añadidos/captura-desde-2024-10-27-10-49-30.png"><br>Cuantificación:<br>
Una vez que la luz ha sido convertida en voltaje, este valor analógico debe convertirse en un valor digital para que pueda procesarse y almacenarse en dispositivos electrónicos. Aquí es donde entra en juego la cuantificación mediante un convertidor analógico a digital (ADC):<br>
<br>Discretización del Voltaje: El rango de voltajes posibles en el sensor se divide en niveles discretos. Por ejemplo, en una imagen de 8 bits, hay 256 niveles posibles (0 a 255), donde cada nivel representa un rango específico de voltajes.<br>

<br>Asignación de Niveles de Intensidad: El ADC asigna a cada píxel un valor numérico correspondiente al nivel de voltaje que ha medido, generando así una representación digital de la imagen. En una imagen en escala de grises de 8 bits, un valor de 0 representaría negro absoluto (sin luz), y 255 representaría blanco absoluto (máxima intensidad de luz).<br>

<br>Profundidad de Bits: La cantidad de niveles de cuantificación está determinada por la profundidad de bits. A mayor profundidad de bits (por ejemplo, 10 bits o 12 bits), más niveles de intensidad se pueden representar, lo que permite una gama de tonos más amplia y una calidad de imagen más detallada. Sin embargo, esto también implica un mayor uso de almacenamiento y potencia de procesamiento.
<br><br>RGB:<br>
Se basa en representar cualquier color con los 3 colores primarios, en su gama de intensidades. Mejor la que el ordenador lo represente.<br><img alt="Pasted image 20241027105317.png" src="añadidos/pasted-image-20241027105317.png"><br>HSV:<br>
Se basa en función de 3 caracteristicas del color, Hue, Saturation y Value.<br>
Mejor interpretado por el ojo humano.<br><img alt="Pasted image 20241027105331.png" src="añadidos/pasted-image-20241027105331.png"><br><br>Un histograma de una imagen es un gráfico de frecuencias de cada uno de los píxeles de la imagen. <br><img alt="Pasted image 20241027105930.png" src="añadidos/pasted-image-20241027105930.png"><br>
<br>Análisis de Contraste:
<br>
<br>
Un histograma que abarca todo el rango de intensidades (desde el negro hasta el blanco) indica una buena distribución del contraste.

<br>
Un histograma concentrado en un extremo sugiere que la imagen podría estar subexpuesta (concentrado a la izquierda) o sobreexpuesta (concentrado a la derecha).

<br>
<br>Ecualización de Histogramas:
<br>
<br>
Este es un método para mejorar el contraste de una imagen al redistribuir los niveles de intensidad.

<br>
Al ecualizar un histograma, se busca que la distribución de los píxeles se extienda a través de todo el rango disponible, lo que resulta en una imagen con mayor contraste y más detalles.

<br>Clasificación de imágenes según su histograma<br>
<br>
Imagen Subexpuesta:<br>
Histograma concentrado a la izquierda, indicando que la mayoría de los píxeles son oscuros.

<br>
Imagen Sobreexpuesta:<br>
Histograma concentrado a la derecha, mostrando que la mayoría de los píxeles son claros.

<br>
Imagen Bien Expuesta:<br>
Histograma que abarca un rango amplio desde la izquierda hasta la derecha, indicando una buena distribución de tonos.

<br><img alt="Captura desde 2024-10-27 11-00-48.png" src="añadidos/captura-desde-2024-10-27-11-00-48.png">]]></description><link>2.-la-imagen-digital.html</link><guid isPermaLink="false">2. La Imagen Digital.md</guid><pubDate>Tue, 29 Oct 2024 18:02:37 GMT</pubDate><enclosure url="añadidos/pasted-image-20241029175542.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;añadidos/pasted-image-20241029175542.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[3. Procesado de la Imagen Digital]]></title><description><![CDATA[ 
 <br>En esta sección hablaremos de algunas técnicas de procesado de la imagen;<br>
en el Dominio Espacial en concreto puntual o con varias imágenes y basado en la vecindad.<br>Antes de comenzar me gustaría aclarar el concepto de Rango Dinámico de una imagen.<br>
<br>
En el contexto de la iluminación en fotografía, el rango dinámico se refiere a la capacidad de captar el detalle en las luces y sombras dentro de una misma imagen. 

<br>
Es la diferencia entre el nivel de luz más intenso y el más débil que una cámara puede capturar sin perder detalles.

<br><br>Normalización del Rango Dinámico:<br>Esta técnica puede ser interesante si se quiere: <br>
<br>Obtener un aumento del contraste para resaltar los detalles tanto de las sombras como de las luces, para tener una imagen más equlibrada y agradable.
<br>Mejora la claridad de la imagen.
<br>Puede mejorar la compatibilidad entre dispositivos con distinto rango dinámico.
<br>Trabajaremos 2 técnicas:<br>Normalización Lineal: Queremos reducir o expandir el rango dinámico.<br><br><br>Normalización de Media y Varianza:<br><br><br>Si fuera a la normal , sería:<br><br>Ajuste de gamma, contraste y brillo:<br>No hay mucho que comentar de esta técnica, mediante una operación píxel a píxel modifica la gamma, constraste y brillo.<br>¿Que es la gamma?<br>La gamma se refiere a un parámetro que describe la relación no lineal entre la intensidad de luz que se muestra en una pantalla (o se captura por un sensor) y la intensidad de luz real (o la señal de entrada). En otras palabras, la gamma define cómo se transforma la señal de voltaje en luminancia percibida.<br><br>
<br>Si , la relación es lineal: la salida del sensor es proporcional a la intensidad de luz .
<br>Si , la respuesta es sub-lineal, lo cual da más importancia a los tonos oscuros y aumenta el detalle en sombras.
<br>Si , la respuesta es sobre-lineal, lo cual aumenta el contraste en las áreas claras.
<br>Ecualización:<br>Esta técnica lo que busca hacer es que a partir del histograma de una imagen, se obtiene su histograma normalizado y acumulado y después aplicar una transformación de los píxeles de este histograma a la imagen real.<br>Esto se hace ya que este histograma esta mucho mejor repartido, se podría decir que sería el histograma de la imagen ideal a la original, ya que no estara ni sobreexpuesta ni subexpuesta.<br><img alt="Captura desde 2024-10-27 11-56-20.png" src="añadidos/captura-desde-2024-10-27-11-56-20.png"><br>LookUp Table:<br>Estas tablas básicamente, para aplicar una conversión de un píxel a otro dentro de una imagen, cada píxel tiene su transformación asociada, normalmente precalculada con el procesamiento que se le quiera aplicar.<br>Las lookup tables hacen mucho más eficiente el procesamiento de la imagen ya que no se tendrá que calcular la operación píxel a píxel de la imagen, ya que ya están precalculadas. <br>Un ejemplo muy sencillo es la inversión de una imagen a cada pixel se aplica , pero al estar precalculado es más eficiente.<br>Cambios de espacio de Color:<br>Los dos sistemas de color que hemos usado han sido por ahora RGB y HSV, es por eso que nos interesa poder cambiar entre estos, por ejemplo si queremos hacer una aplicación que aplique Chroma Key nos interesa cambiar al espacio de color HSV para tener el rango de colores del cual deseamos crear la máscara.<br>Balanceo de Color:<br>Trabajamos 2 técnicas: White Patch y Gray World.<br>La primera técnica White Patch, consiste en que el punto más blanco de una imagen debe ser el blanco, dependiendo de algunos parámetros lo que se suele hacer es buscar el punto más brillante de la imagen y reescalar esta para que el punto más brillante sea e blanco, otras versiones y para evitar blancos quemados, se usa la media se los p% puntos menos brillante.<br>En el caso del Gray World, lo que queremos obtener es neutralizar los dominantes de color, haciendo una imagen más neutra (más cercana al gris), de nuevo aplicando un reescalado.<br><br>
<br>
Suma: La suma de numerosas imágenes iguales con ruido hace que la imagen, gane nitidez.

<br>
Resta: Sirve principalmente para detectar movimientos en planos estáticos, si hacemos la diferencia de dos fotos de la misma escena y algo se ha movido, la diferencia saldrá todo en negro (0) menos la silueta del elemento distinto.

<br>
Multiplicación/División: pueden servir para reducir iluminaciones no deseadas dividiendo  imágenes mal iluminadas entre una imagen tipo que represente los tonos de luz que no queremos.

<br>
Operaciones Lógicas: Útiles para la composición de imágenes.

]]></description><link>3.-procesado-de-la-imagen-digital.html</link><guid isPermaLink="false">3. Procesado de la Imagen Digital.md</guid><pubDate>Sun, 27 Oct 2024 11:21:39 GMT</pubDate><enclosure url="añadidos/captura-desde-2024-10-27-11-56-20.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;añadidos/captura-desde-2024-10-27-11-56-20.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[4. Procesado Basado en la Vecindad]]></title><description><![CDATA[ 
 <br>En el procesamiento de imágenes, la vecindad de un píxel se refiere al conjunto de píxeles circundantes que rodean a un píxel central específico dentro de una imagen. <br>Estos píxeles vecinos son considerados cuando se aplican ciertos filtros y operaciones de procesamiento de imágenes que dependen del contexto local de cada píxel, como en operaciones de suavizado, detección de bordes y reducción de ruido.<br><br>La interpolación es una técnica para estimar valores de píxeles en posiciones que no coinciden con un píxel exacto de la imagen original. Es útil en operaciones como cambio de tamaño, rotación y distorsión de imágenes. <br>A continuación, se presentan tres métodos comunes: Vecino Más Cercano (NN), Bilineal y Bicúbica.<br>Vecino Más Cercano (Nearest Neighbor - NN):<br>Este es el método más simple y rápido, que asigna a un píxel el valor del píxel más cercano de la imagen original.<br>
<br>Método: Se busca el píxel en la imagen original más cercano a la posición en la imagen de salida y se asigna su valor al píxel de salida.
<br>Ventajas: Muy rápido y eficiente en cuanto a cómputo.
<br>Desventajas: Produce bordes "escalonados" o pixelados debido a la falta de suavidad entre píxeles.
<br>Aplicaciones: Útil en aplicaciones donde la velocidad es prioritaria, como en visualización rápida o en segmentación de imágenes.
<br><img alt="Captura desde 2024-10-27 21-21-48.png" src="añadidos/captura-desde-2024-10-27-21-21-48.png"><br>Interpolación Bilineal:<br>La interpolación bilineal considera no solo el píxel más cercano, sino también los píxeles vecinos inmediatos en direcciones horizontales y verticales. Calcula el valor del píxel de salida como una combinación ponderada de estos cuatro píxeles.<br>
<br>
Método:

<br>Encuentra los cuatro píxeles vecinos de la posición en la imagen original.
<br>Realiza interpolaciones lineales en las direcciones horizontal y vertical, promediando los valores según la distancia.


<br>
Ventajas: Transiciones más suaves entre píxeles, lo que reduce el efecto de pixelado.

<br>
Desventajas: Requiere más cálculos, lo cual puede aumentar el tiempo de procesamiento.

<br>
Aplicaciones: Ideal para redimensionamiento y rotación donde se busca mejorar la calidad visual sin mucha carga de procesamiento.

<br><img alt="Captura desde 2024-10-27 21-22-27.png" src="añadidos/captura-desde-2024-10-27-21-22-27.png"><br>Interpolación Bicúbica:<br>Este método avanzado usa 16 píxeles vecinos (4x4) y se basa en funciones cúbicas para transiciones más suaves y precisas.<br>
<br>
Método:

<br>Toma un bloque de 4x4 píxeles alrededor del píxel de interés.
<br>Calcula interpolación cúbica en cada dirección (horizontal y vertical) para una estimación precisa del píxel de salida.


<br>
Ventajas: Alta calidad visual, con transiciones muy suaves, ideal para imágenes con detalles finos.

<br>
Desventajas: Es el método más lento de los tres debido al número de cálculos necesarios, y puede introducir artefactos de suavizado.

<br>
Aplicaciones: Usado en aplicaciones donde se prioriza la calidad de imagen, como en edición de fotografías, impresión de alta calidad y gráficos computacionales.

<br><img alt="Captura desde 2024-10-27 21-23-02.png" src="añadidos/captura-desde-2024-10-27-21-23-02.png"><br><br><img alt="Captura desde 2024-10-27 21-23-25.png" src="añadidos/captura-desde-2024-10-27-21-23-25.png"><br>
<img alt="Captura desde 2024-10-27 21-23-51.png" src="añadidos/captura-desde-2024-10-27-21-23-51.png"><br><br>Correlación:<br><br>Básicamente por cada píxel de la imagen, extraemos una parcela del tamaño del filtro, multiplicamos en cada píxel de la parcela lo valores del filtro y suma esos valores al píxel en el que estamos trabajando.<br>Donde:<br>
 : es la imagen resultado.<br>
 : es la imagen de entrada.<br>
 : es el filtro.<br>La correlación se utiliza para buscar patrones y coincidencias.<br>Convolución:<br><br>Hacemos la misma operación que con la correlación solo que el filtro esta rotado 180º.<br>La convolución se suele aplicar  como desenfoque, detección de bordes y suavizado.<br>Vamos a ver algunos tipos de filtros lineales.<br>Filtro Paso Baja (Box Filter):<br><br>Bloquea las altas frecuencias para quedarnos con un resultado<br>
suavizado, con estructuras principales y sin detalles.<br>Filtro Paso Baja (Gaussian Filter):<br><br><br>Mismo objetivo que el anterior. Le da más importancia a los píxeles que comparten aristas con el seleccionado a estudio que a las diagonales. <br>Utiliza una distribución gaussiana con una constante  a elegir. <br>A mayor  la imagen obtenida es de más baja frecuencia.<br><img alt="Captura desde 2024-10-28 02-01-02.png" src="añadidos/captura-desde-2024-10-28-02-01-02.png"><br>Realce de Imagen (Unsharp Mask):<br>El Unsharp Mask (máscara de enfoque) es una técnica utilizada en el procesamiento de imágenes para mejorar la nitidez de una imagen. <br>A pesar de su nombre, el unsharp mask no hace que una imagen se vea menos nítida; en realidad, aumenta la claridad de los bordes y los detalles, creando la ilusión de que la imagen es más nítida.<br>Realiza la siguiente operación:<br><br>o también:<br><br>Donde:<br>
 : es la imagen de salida enfocada.<br>
 : es la imagen original.<br>
 : es un coeficiente que si es mayor que 1 se denomina high-boost-filtering.<br>
 : imagen desenfocada o suavizada.<br>A mayor  mayor enfoque.<br><img alt="Captura desde 2024-10-28 02-14-30.png" src="añadidos/captura-desde-2024-10-28-02-14-30.png"><br>Filtros Paso Alta (Derivadas de la imagen):<br>Las derivadas de una imagen son herramientas fundamentales en el procesamiento de imágenes, ya que permiten medir el cambio de intensidad en diferentes direcciones.<br>Esto es especialmente útil para detectar bordes, que son transiciones rápidas en la intensidad de píxeles.<br>La primera derivada indica el cambio de intensidad en el eje <br><br>
<br>Cero en las zonas constantes
<br>Distinto de cero en el escalón o comienzo/fin de rampas
<br>No cero durante la rampa
<br>Kernel para primera derivada:<br><br>La segunda derivada da información de la curvatura de la intensidad.<br><br>
<br>Cero en las zonas constantes
<br>Distinto de cero en las zonas de comienzo/fin de rampa
<br>Cero durante la rampa
<br>Kernel para segunda derivada:<br><br><img alt="Captura desde 2024-10-28 02-23-38.png" src="añadidos/captura-desde-2024-10-28-02-23-38.png"><br>Filtros Paso Alta (Filtro Sobel):<br>El filtro Sobel es una técnica comúnmente utilizada en el procesamiento de imágenes para la detección de bordes. <br>Se basa en calcular la derivada de la intensidad de la imagen en direcciones específicas (horizontal y vertical) para resaltar los cambios abruptos en la intensidad de los píxeles.<br>¿Como funciona?<br>El filtro Sobel utiliza dos matrices de convolución (kernels) que se aplican a la imagen para calcular la magnitud y la dirección del gradiente de la intensidad de la imagen.<br>Estas matrices están diseñadas para resaltar bordes y cambios de intensidad en las direcciones horizontal y vertical.<br><br><br>La magnitud del gradiente se puede calcular con:<br><br>Filtros Paso Alta (Laplaciano):<br>El operador Laplaciano es un filtro utilizado en el procesamiento de imágenes para detectar bordes y cambios abruptos en la intensidad de una imagen. <br>Se puede considerar un operador de paso alto porque realza las áreas donde hay cambios significativos, mientras que suprime las áreas donde la intensidad es constante.<br>Se representa con:<br><br>Sus kernels más representativos son:<br><br>Esta es otra representación que resalta los bordes de manera más intensa:<br><br>Realce de Imagen (Sharpenning):<br>El realce de imágenes o sharpening es un conjunto de técnicas utilizadas para mejorar la claridad y definición de los detalles en una imagen. <br>El objetivo principal del sharpening es resaltar los bordes y otros detalles significativos, haciendo que la imagen parezca más nítida.<br>Sus kernels más representativos:<br>Este kernel realza el píxel central mientras suprime los píxeles vecinos.<br><br>Este kernel es más agresivo en la detección de bordes.<br><br>Se deducen estos kernels de la siguiente interpretación del sharpenning:<br><br><br>En el filtrado no lineal, el valor de cada píxel se calcula mediante una operación no lineal basada en los valores de los píxeles vecinos, lo que permite resultados más adaptativos y robustos para ciertos tipos de ruido<br>Filtro por Mediana:<br>Sustituye el valor de cada píxel por la mediana de los valores de su vecindad.<br>Es especialmente útil para eliminar el ruido impulsivo o también llamado ruido de sal y pimienta.<br><img alt="Captura desde 2024-10-28 03-15-03.png" src="añadidos/captura-desde-2024-10-28-03-15-03.png"><br>Filtro Bilateral:<br>El filtro bilateral es un tipo de filtro no lineal que se utiliza en el procesamiento de imágenes para suavizar áreas sin afectar significativamente los bordes. Es muy popular para reducir el ruido en las imágenes, especialmente cuando se quiere preservar los detalles y bordes importantes.<br>A diferencia de otros filtros de suavizado, el filtro bilateral no solo considera la cercanía espacial de los píxeles, sino también la similitud de intensidad entre los píxeles vecinos, lo cual le permite diferenciar entre áreas con variación de intensidad gradual (como el interior de un objeto) y bordes definidos (donde hay un cambio brusco de intensidad).<br><br><img alt="Captura desde 2024-10-28 03-21-55.png" src="añadidos/captura-desde-2024-10-28-03-21-55.png"><br>Morfología (Operadores):<br>La erosión es un operador que reduce el tamaño de los objetos en la imagen. El objetivo es "erosionar" las fronteras de los objetos, lo que reduce el ruido y separa objetos que están ligeramente conectados.<br>
<br>
Definición: La erosión de una imagen  por un elemento estructurante  consiste en reducir las zonas de intensidad alta (blancas) en la imagen. En una imagen binaria, un píxel permanecerá en el valor alto (1 o blanco) solo si todos los píxeles de la vecindad definida por el elemento estructurante también son 1. Si algún píxel en la vecindad es 0 (negro), entonces el píxel central se convertirá en 0.

<br>
Efecto Visual: La erosión tiende a adelgazar las áreas blancas de la imagen y a eliminar pequeñas regiones y ruidos aislados.

<br>La dilatación es el operador opuesto a la erosión. En este caso, se expanden las áreas de intensidad alta (blancas) en la imagen. Este operador es útil para rellenar huecos en objetos, unir componentes vecinos y agrandar objetos en la imagen.<br>
<br>Definición: La dilatación de una imagen  por un elemento estructurante  consiste en expandir las zonas de intensidad alta (blancas). En una imagen binaria, un píxel se convierte en 1 si algún píxel en la vecindad definida por el elemento estructurante es 1.<br>

<br>Efecto Visual: La dilatación aumenta el tamaño de las áreas blancas y tiende a unir pequeñas regiones que están próximas, rellenando espacios entre objetos.
<br>En definitiva y lo importante que te tienes que quedar, es que en la dilatación te quedas con el píxel activo más grande y en la erosión con el más pequeño.<br><img alt="Captura desde 2024-10-28 03-28-51.png" src="añadidos/captura-desde-2024-10-28-03-28-51.png"><br>]]></description><link>4.-procesado-basado-en-la-vecindad.html</link><guid isPermaLink="false">4. Procesado Basado en la Vecindad.md</guid><pubDate>Tue, 29 Oct 2024 07:53:51 GMT</pubDate><enclosure url="añadidos/captura-desde-2024-10-27-21-21-48.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;añadidos/captura-desde-2024-10-27-21-21-48.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[5. Modelo Proyectivo]]></title><description><![CDATA[ 
 <br>En está sección se nos plantea el siguiente problema que también vimos en el bloque anterior y nos da los fundamentos para después entender como se reconstruye la escena 3D.<br><br>¿Como recupero un punto 3D de una imagen?<br>Como bien ya sabes al hacer una fotografía la componente de la profundidad se pierde, sin embargo, esta es muy importante.<br><img alt="Pasted image 20241029174230.png" src="añadidos/pasted-image-20241029174230.png"><br>Te recomiendo que para repasar esta parte mires esta explicación en <a data-href="2. La Imagen Digital" href="2.-la-imagen-digital.html" class="internal-link" target="_self" rel="noopener nofollow">2. La Imagen Digital</a>.<br>La novedad es que WOW, podemos representar esto como una matriz.<br><br>El vector resultante estará en coordenadas homogéneas, para obtener el vector no homogéneo se divide por la tercer componente.<br><br>Sin embargo, esto solo funciona en una situación ideal y presenta los siguientes problemas:<br>
<br>
Distorsión de perspectiva: Aunque la proyección en perspectiva es deseable para algunas aplicaciones, no representa la realidad con suficiente precisión cuando se usan lentes reales.

<br>
No considera distorsiones que se producen en cámaras reales, especialmente en cámaras con lentes (como la distorsión radial o tangencial).

<br>
Calibración: El modelo pinhole es demasiado ideal para aplicaciones que requieren geometría precisa, como en visión artificial o reconstrucción 3D.

<br><img alt="Pasted image 20241029191812.png" src="añadidos/pasted-image-20241029191812.png"><br><br>Para mejorar la precisión del modelo pinhole, se introduce el modelo pinhole mejorado, que incluye parámetros adicionales para reflejar mejor las características y distorsiones reales de una cámara. Estos parámetros son esenciales para corregir el modelo básico en aplicaciones que requieren alta precisión, como la visión artificial o la reconstrucción 3D.<br>Mejoras del modelo Pin-Hole:<br>
<br>
Distorsión Radial: Debido a la curvatura de las lentes, los puntos en el borde de la imagen se ven distorsionados, especialmente aquellos alejados del centro óptico. Esta distorsión se modela mediante parámetros , , y en algunos casos , que ajustan el efecto de la distorsión radial en el modelo mejorado.


donde  representa la distancia desde el centro óptico al punto proyectado en la imagen.

<br><img alt="Pasted image 20241029192401.png" src="añadidos/pasted-image-20241029192401.png"><br>
<br>
Distorsión Tangencial: Esta distorsión ocurre debido al desalineamiento de los elementos en la lente. Se modela con los parámetros  y :



<br><img alt="Pasted image 20241029192458.png" src="añadidos/pasted-image-20241029192458.png"><br>
<br>
Parámetros Intrínsecos de la Cámara: Además de las distorsiones, el modelo mejorado incluye parámetros intrínsecos que especifican el centro óptico  y el factor de escala de la imagen en los ejes  y , es decir,  y . Estos parámetros permiten modelar variaciones en la distancia focal en los ejes horizontales y verticales de la cámara.
La matriz de calibración de la cámara es:

donde:

<br> y  representan las distancias focales en los ejes  y  respectivamente,
<br> es el centro de la imagen en el plano de imagen.


<br>Ecuación Final del Modelo Mejorado<br>La proyección de un punto  en el espacio 3D hacia un punto  en la imagen, considerando tanto los efectos de distorsión como los de calibración de cámara, es:<br><br><br>donde se aplican las correcciones de distorsión radial y tangencial para obtener las coordenadas corregidas .<br>Este modelo avanzado compensa los efectos no ideales de las lentes y la cámara, proporcionando una representación de la proyección más precisa y ajustada a la geometría de la cámara real.<br><br>Los parámetros extrínsecos de una cámara definen su posición y orientación en relación al sistema de referencia global. Es decir, describen cómo se encuentra ubicada y orientada la cámara en el espacio 3D en relación a un marco de referencia, permitiéndonos transformar las coordenadas de un punto en el mundo a las coordenadas de la cámara.<br>¿Por Qué Son Importantes los Parámetros Extrínsecos?<br>
Cuando se proyecta un punto desde el espacio 3D hacia una imagen 2D, necesitamos saber desde dónde y en qué dirección está "mirando" la cámara. Los parámetros extrínsecos nos permiten alinear el sistema de coordenadas de la cámara con el sistema de referencia del mundo, esencial en aplicaciones como:<br>
<br>Reconstrucción 3D: para mapear correctamente puntos en el mundo real.
<br>Seguimiento de objetos: para conocer el movimiento de un objeto relativo a la cámara.
<br>Realidad aumentada: para alinear objetos virtuales con el entorno real desde la perspectiva de la cámara.
<br>Desglose de los Parámetros Extrínsecos<br>
<br>
Matriz de Rotación ():

<br>Una matriz de tamaño  que representa la orientación de la cámara en relación al sistema de referencia global.
<br>Aplica una rotación a los puntos del sistema de coordenadas del mundo para alinearlos con la orientación de la cámara.

La matriz de rotación tiene la forma:


<br>
Vector de Traslación ():

<br>Un vector  que indica la posición de la cámara en el espacio 3D en relación al sistema de referencia global.
<br>Traslada el sistema de coordenadas de la cámara de forma que el origen del sistema de referencia global quede en el lugar correcto relativo a la cámara.

El vector de traslación tiene la forma:


<br> Transformación Completa: De Coordenadas del Mundo a Coordenadas de la Cámara<br>
Para convertir un punto en el espacio del mundo  a las coordenadas de la cámara , usamos la siguiente fórmula:<br><br>donde  aplica la rotación y  aplica la traslación. Expandiendo esto:<br><br>Esto transforma un punto  en coordenadas del mundo a coordenadas de la cámara , las cuales se pueden usar luego para la proyección al plano de la imagen.<br>Matriz de Transformación Extrínseca Completa<br>
Combinando la rotación y la traslación en una sola matriz  llamada matriz de transformación extrínseca, podemos escribir:<br><br>donde:<br><br>Esta matriz  permite transformar cualquier punto 3D del espacio del mundo al espacio de la cámara en una única multiplicación de matrices, esencial para realizar transformaciones consistentes y precisas en el análisis de imágenes y aplicaciones en 3D.<br><br>La calibración de la cámara es el proceso de determinar los parámetros intrínsecos y parámetros extrínsecos que relacionan un sistema de coordenadas del mundo con el sistema de la cámara y finalmente con el plano de la imagen. La calibración permite proyectar puntos en el espacio 3D a puntos en la imagen de manera precisa, lo cual es esencial en aplicaciones de visión artificial, reconstrucción 3D y mapeo.<br>El resultado que obtenemos de los parámetros intrísecos y extrínsecos es la matriz proyectiva:<br><br>Lo que hace que encontrar las coordenadas en la imagen se haga de la siguiente manera:<br><br><br>Para el siguiente tema es interesante tener en cuenta esto:<br><br>Donde:<br>
<br> la proyección del punto 3D en la imagen
<br> es la matriz proyectiva
<br> es el vector de coordenadas homogéneas del punto 3D
<br> es la pseudo-inversa de 
<br> es el vector de dirección al punto 3D, (Al ser la pseudo-inversa no te da la magnitud exacta solo la dirección)
<br> es la ecuación de la recta desde el centro óptico de la cámara hasta el punto 3D
<br> es el centro óptico de la cámara
<br> es un parámetro escalar que varía a lo largo de la recta
<br><img alt="Pasted image 20241108180646.png" src="añadidos/pasted-image-20241108180646.png"><br>Se suele utilizar un método que usa "Tableros de Ajedrez", mediante un algoritmo reconoce las esquinas de este tablero y se obtienen todos los parámetros instrínsecos (Matriz de intrinseca , Coeficientes de distorsión ) y extrínsecos (Matriz de Rotación y traslación )<br><img alt="Pasted image 20241108181814.png" src="añadidos/pasted-image-20241108181814.png"><br>Aqui un ejemplo de código de como se hace el algoritmo en python:<br>import cv2
import numpy as np

# Tamaño del tablero (número de esquinas interiores)
pattern_size = (9, 6)  # 9x6 es común, ajústalo según tu tablero

# Tamaño de cada cuadrado en el tablero (por ejemplo, 25 mm)
square_size = 25

# Preparar puntos 3D en el espacio del mundo (0,0,0), (1,0,0), ..., (8,5,0)
objp = np.zeros((pattern_size[0]*pattern_size[1], 3), np.float32)
objp[:,:2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)
objp *= square_size

# Listas para almacenar puntos 3D y 2D
objpoints = []  # Puntos 3D en el espacio del mundo
imgpoints = []  # Puntos 2D en el plano de la imagen

# Cargar imágenes de calibración
images = [f'image{i}.jpg' for i in range(1, 21)]  # Suponiendo 20 imágenes

for fname in images:
    img = cv2.imread(fname)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Encontrar las esquinas del tablero de ajedrez
    ret, corners = cv2.findChessboardCorners(gray, pattern_size, None)

    if ret:
        objpoints.append(objp)
        # Refinar ubicaciones de las esquinas
        corners2 = cv2.cornerSubPix(gray, corners, (11,11), (-1,-1), 
                                    criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001))
        imgpoints.append(corners2)

        # Dibujar y mostrar las esquinas detectadas (opcional)
        img = cv2.drawChessboardCorners(img, pattern_size, corners2, ret)
        cv2.imshow('Corners', img)
        cv2.waitKey(100)

cv2.destroyAllWindows()

# Calibración de la cámara
ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)

# Imprimir los parámetros obtenidos
print("Matriz intrínseca:\n", mtx)
print("Coeficientes de distorsión:\n", dist)
print("Vectores de rotación:\n", rvecs)
print("Vectores de traslación:\n", tvecs)

]]></description><link>5.-modelo-proyectivo.html</link><guid isPermaLink="false">5. Modelo Proyectivo.md</guid><pubDate>Tue, 12 Nov 2024 10:14:26 GMT</pubDate><enclosure url="añadidos/pasted-image-20241029174230.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;añadidos/pasted-image-20241029174230.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[6. Reconstrucción 3D]]></title><description><![CDATA[ 
 <br>En este apartado de la asignatura veremos algunas de las técnicas para recuperar la información 3D de las escena.<br>La idea principal o la primera idea sobre este tema es que, con una imagen sola no basta para poder obtener la perspectiva de una escena y menos poder reconstruirla en 3D, ya que una imagen es solo una cara 2D de la realidad.<br>Con dos podemos encontrar una correspondencia y a partir de varias de estas reconstruir la imagen, se puede entender también como triangular el punto 3D.<br><img alt="Pasted image 20241112113008.png" src="añadidos/pasted-image-20241112113008.png"><br><br>Esto en la práctica no siempre es eficiente ya que las rectas proyectivas de las vistas al punto 3D no siempre van a coincidir, debido al ruido debemos aproximar con la mayor precisión posible que las rectas proyectivas intersecten.<br><img alt="Pasted image 20241112113428.png" src="añadidos/pasted-image-20241112113428.png"><br><br>Epipolos:<br>
<br>Cada cámara tiene un epipolo, que es el punto en el plano de la imagen donde la línea que conecta los centros ópticos de las cámaras (la línea base) intersecta dicho plano de la imagen.
<br>Para dos cámaras, existe un epipolo en cada imagen: el epipolo derecho en la imagen de la cámara derecha y el epipolo izquierdo en la imagen de la cámara izquierda.
<br>Linea Epipolar:<br>
Debido a que ambas cámaras forman un plano, si queremos encontrar un punto de una imagen en la otra estará su correspondencia en la linea epipolar, esto reduce la complejidad de la búsqueda de  a .<br><img alt="Pasted image 20241112154018.png" src="añadidos/pasted-image-20241112154018.png"><br>Plano Epipolar:<br>
<br>El plano epipolar es el plano que contiene un punto en el espacio, así como los centros de proyección (los centros ópticos) de ambas cámaras.
<br>Este plano se proyecta como líneas epipolares en cada imagen.
<br><img alt="Pasted image 20241112153543.png" src="añadidos/pasted-image-20241112153543.png"><br><br>La visión estéreo es un método específico para la reconstrucción 3D que utiliza dos cámaras (o dos puntos de vista diferentes de una cámara) para simular cómo perciben la profundidad los humanos.<br>
<br>Par de Imágenes Estéreo: 
<br>
<br>La visión estéreo se basa en obtener un par de imágenes de la misma escena desde dos puntos de vista ligeramente diferentes.
<br>La disparidad entre los puntos correspondientes en cada imagen se puede usar para calcular la profundidad de cada punto en la escena.
<br>
<br>Mapeo de Disparidad:
<br>
<br>La disparidad es la diferencia en la posición de un punto en las dos imágenes. 
<br>A partir de la disparidad, y conociendo la distancia entre las cámaras (conocida como base estéreo), se puede calcular la profundidad de los puntos en la escena usando:
<br><img alt="Pasted image 20241112153318.png" src="añadidos/pasted-image-20241112153318.png"><br> donde: <br>
<br> es la profundidad, 
<br> es la distancia focal de la cámara,
<br> es la distancia entre las cámaras (base estéreo). 
<br>La disparidad es inversamente proporcional a la distancia, es decir a mayor distancia, menor disparidad y viceversa.<br><img alt="Pasted image 20241112154950.png" src="añadidos/pasted-image-20241112154950.png"><br>
<br>Correspondencia de Puntos: 
<br>
<br>
Uno de los principales desafíos en la visión estéreo es encontrar correspondencias exactas de puntos entre las dos imágenes.

<br>
Técnicas como el algoritmo de bloqueo, SAD (Suma de Diferencias Absolutas) o SIFT (Transformada Invariante de Escala) son algunas de las más comunes para este fin. 

<br>
<br>
Horópteros y Rango de Disparidad:
El horóptero es un concepto en visión estereoscópica que se refiere al conjunto de puntos en el espacio que, al proyectarse en los dos ojos o en un sistema de cámaras estéreo, generan imágenes en la misma posición relativa en ambas retinas (o sensores de las cámaras) sin ninguna disparidad. 
En otras palabras, los puntos en el horóptero tienen una disparidad cero.

<br>Puntos dentro del Rango de Disparidad:


<br>
Los puntos cercanos al horóptero, que presentan disparidades pequeñas (positivas o negativas) en función de la distancia con respecto al horóptero, caen dentro de este rango de disparidad.

<br>
Estos puntos están en un área conocida como el volumen de Panum en percepción humana, o simplemente el volumen de profundidad estéreo en visión artificial



<br>Puntos Fuera del Rango de Disparidad:


<br>Los puntos más allá del rango de disparidad máximo que el sistema puede procesar generan disparidades demasiado grandes para ser manejadas adecuadamente. Estos puntos aparecen desenfocados o sin correspondencia clara en las imágenes estéreo, dificultando la reconstrucción precisa de su profundidad.


<br>Relación con la Precisión en Estimación de Profundidad:


<br>
Dentro del rango de disparidad, el sistema estéreo tiene una precisión aceptable para estimar la profundidad de los puntos.

<br>
A medida que un punto se aleja del horóptero, su disparidad aumenta, y la precisión en la estimación de profundidad disminuye. Esto se debe a la relación inversa entre disparidad y profundidad: mayores disparidades se asocian con distancias más cercanas al sistema de cámaras, mientras que menores disparidades representan puntos más lejanos.
<img alt="Pasted image 20241112164926.png" src="añadidos/pasted-image-20241112164926.png">



<br><br>Ya sea por la luz o las condiciones que se tomaran las imágenes estas no podrán ser procesadas como si fuera en una situación ideal, también debido al ruido.<br>Es por eso que se hace un proceso de rectificación de la imagen para acercarla lo máximo posible a la situación ideal.<br>Se realiza el siguiente proceso:<br>El proceso de rectificación estéreo sigue generalmente estos pasos:<br>
<br>
Calibración de las Cámaras:

<br>Antes de rectificar, es necesario realizar una calibración de la cámara para obtener tanto los parámetros intrínsecos (focal, centro óptico) como los parámetros extrínsecos (rotación y traslación relativa entre las cámaras).
<br>Los parámetros extrínsecos son esenciales para entender cómo están orientadas y posicionadas las cámaras entre sí y en relación al mundo, ya que esto influye directamente en cómo se deben rectificar las imágenes.


<br>
Estimación de la Matriz de Rotación y Traslación:

<br>Con los parámetros de calibración, se calculan las matrices de rotación y traslación necesarias para alinear los planos de imagen de ambas cámaras de modo que sus ejes estén paralelos.
<br>Esta rotación reorienta las imágenes de manera que los pares de puntos de ambas cámaras se proyecten en las mismas líneas horizontales.

<img alt="Pasted image 20241112170655.png" src="añadidos/pasted-image-20241112170655.png">

<br>
Cálculo de las Nuevas Matrices de Proyección:

<br>Se generan nuevas matrices de proyección para ambas cámaras rectificadas, que aseguran que las imágenes estén alineadas y que los puntos homólogos se encuentren en la misma fila de cada imagen.
<br>Esto transforma las imágenes originales en imágenes "rectificadas", donde las correspondencias son más fáciles de encontrar.


<br>
Transformación de las Imágenes:

<br>Se aplica una transformación de perspectiva (usando las matrices de proyección calculadas) a las imágenes para crear las versiones rectificadas.
<br>Como resultado, los puntos correspondientes se encuentran ahora en la misma línea horizontal en ambas imágenes, facilitando el análisis estéreo.

<img alt="Pasted image 20241112171010.png" src="añadidos/pasted-image-20241112171010.png"> 
<img alt="Pasted image 20241112171710.png" src="añadidos/pasted-image-20241112171710.png">

<br>Uso del  para la búsqueda de la correspondencia:<br>También se usa para la correspondencia como antes mencione el SAD.<br><br>Lo que reduce la búsqueda de correspondencias a una dimensión.<br>
<br>Primero se procesa la imagen para normalizar el brillo y resaltar textura.
<br>Se aplica 
<br>Y por último se filtra para eliminar malas correspondencias.
<br><img alt="Pasted image 20241112172306.png" src="añadidos/pasted-image-20241112172306.png"><br><img alt="Pasted image 20241112172318.png" src="añadidos/pasted-image-20241112172318.png"><br><br><img alt="Pasted image 20241112172443.png" src="añadidos/pasted-image-20241112172443.png">]]></description><link>6.-reconstrucción-3d.html</link><guid isPermaLink="false">6. Reconstrucción 3D.md</guid><pubDate>Tue, 12 Nov 2024 16:24:44 GMT</pubDate><enclosure url="añadidos/pasted-image-20241112113008.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;añadidos/pasted-image-20241112113008.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[index]]></title><description><![CDATA[ 
 <br><br><a data-href="1. Módulo de Adquisición" href="1.-módulo-de-adquisición.html" class="internal-link" target="_self" rel="noopener nofollow">1. Módulo de Adquisición</a><br>
<a data-href="2. La Imagen Digital" href="2.-la-imagen-digital.html" class="internal-link" target="_self" rel="noopener nofollow">2. La Imagen Digital</a><br>
<a data-href="3. Procesado de la Imagen Digital" href="3.-procesado-de-la-imagen-digital.html" class="internal-link" target="_self" rel="noopener nofollow">3. Procesado de la Imagen Digital</a><br>
<a data-href="4. Procesado Basado en la Vecindad" href="4.-procesado-basado-en-la-vecindad.html" class="internal-link" target="_self" rel="noopener nofollow">4. Procesado Basado en la Vecindad</a><br><br><a data-href="5. Modelo Proyectivo" href="5.-modelo-proyectivo.html" class="internal-link" target="_self" rel="noopener nofollow">5. Modelo Proyectivo</a><br>
<a data-href="6. Reconstrucción 3D" href="6.-reconstrucción-3d.html" class="internal-link" target="_self" rel="noopener nofollow">6. Reconstrucción 3D</a>]]></description><link>index.html</link><guid isPermaLink="false">index.md</guid><pubDate>Tue, 12 Nov 2024 10:19:41 GMT</pubDate></item></channel></rss>